{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Main.py"
      ],
      "metadata": {
        "id": "iDnCniYKo-vs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3jrGmP6n-xe"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "from A1_submission import logistic_regression, tune_hyper_parameter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "\n",
        "def compute_score(acc, acc_thresh):\n",
        "    min_thres, max_thres = acc_thresh\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit.py"
      ],
      "metadata": {
        "id": "ApA34WX-pgJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyIterableDataset(MNIST_dataset):\n",
        "  def __init__(self, start, end):\n",
        "      super(MyIterableDataset).__init__()\n",
        "      assert end > start,\n",
        "      self.start = start\n",
        "      self.end = end\n",
        "\n",
        "  def __iter__(self):\n",
        "    return iter(range(self.start, self.end))\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    dataset = worker_info.dataset  # the dataset copy in this worker process\n",
        "    overall_start = dataset.start\n",
        "    overall_end = dataset.end\n",
        "    # configure the dataset to only process the split workload\n",
        "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
        "    worker_id = worker_info.id\n",
        "    dataset.start = overall_start + worker_id * per_worker\n",
        "    dataset.end = min(dataset.start + per_worker, overall_end)\n",
        "\n"
      ],
      "metadata": {
        "id": "SGKO-F7GsV1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_training = torchvision.datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "MNIST_test_set = torchvision.datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "\n",
        "\n",
        "# create a training and a validation set\n",
        "\n",
        "MNIST_training_set,MNIST_validation_set = random_split(MNIST_training,[len(MNIST_training)-12000,12000])\n",
        "\n",
        "\n",
        "#MNIST_training_set, MNIST_validation_set = random_split(MNIST_training, [55000, 5000])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MNIST_training_set,batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(MNIST_validation_set,batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "6bZYQnqaqYZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def logistic_regression(dataset_name, device):\n",
        "    # TODO: implement logistic regression here\n",
        "    results = dict(\n",
        "        model=None\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def tune_hyper_parameter(dataset_name, target_metric, device):\n",
        "    # TODO: implement logistic regression hyper-parameter tuning here\n",
        "    best_params = best_metric = None\n",
        "\n",
        "    return best_params, best_metric\n"
      ],
      "metadata": {
        "id": "E3UPF7LepBl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(\n",
        "        model,\n",
        "        dataset_name,\n",
        "        device,\n",
        "\n",
        "):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        test_dataset = datasets.MNIST(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        test_dataset = datasets.CIFAR10(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "    else:\n",
        "        raise AssertionError(f\"Invalid dataset: {dataset_name}\")\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "            predicted = torch.argmax(output, dim=1)\n",
        "            total += targets.size(0)\n",
        "            num_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    acc = float(num_correct) / total\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "_5wyl2J0ovpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    \"\"\"\n",
        "    command-line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    'MNIST': run on MNIST dataset (part 1)\n",
        "    'CIFAR10': run on CIFAR10 dataset (part 2)\n",
        "    \"\"\"\n",
        "    dataset = \"MNIST\"\n",
        "    # dataset = \"CIFAR10\"\n",
        "\n",
        "    \"\"\"\n",
        "    'logistic': run logistic regression on the specified dataset (parts 1 and 2)\n",
        "    'tune': run hyper parameter tuning (part 3)\n",
        "    \"\"\"\n",
        "    mode = 'logistic'\n",
        "    # mode = 'tune'\n",
        "\n",
        "    \"\"\"\n",
        "    metric with respect to which hyper parameters are to be tuned\n",
        "    'acc': validation classification accuracy\n",
        "    'loss': validation loss\n",
        "    \"\"\"\n",
        "    target_metric = 'acc'\n",
        "    # target_metric = 'loss'\n",
        "\n",
        "    \"\"\"\n",
        "    set to 0 to run on cpu\n",
        "    \"\"\"\n",
        "    gpu = 1"
      ],
      "metadata": {
        "id": "Ex4lcyK-o31U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    args = Args()\n",
        "    try:\n",
        "        import paramparse\n",
        "        paramparse.process(args)\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    acc_thresh = dict(\n",
        "        MNIST=(0.84, 0.94),\n",
        "        CIFAR10=(0.30, 0.40),\n",
        "    )\n",
        "\n",
        "    if args.mode == 'logistic':\n",
        "        start = timeit.default_timer()\n",
        "        results = logistic_regression(args.dataset, device)\n",
        "        model = results['model']\n",
        "\n",
        "        if model is None:\n",
        "            print('model is None')\n",
        "            return\n",
        "\n",
        "        stop = timeit.default_timer()\n",
        "        run_time = stop - start\n",
        "\n",
        "        accuracy = test(\n",
        "            model,\n",
        "            args.dataset,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        score = compute_score(accuracy, acc_thresh[args.dataset])\n",
        "        result = OrderedDict(\n",
        "            accuracy=accuracy,\n",
        "            score=score,\n",
        "            run_time=run_time\n",
        "        )\n",
        "        print(f\"result on {args.dataset}:\")\n",
        "        for key in result:\n",
        "            print(f\"\\t{key}: {result[key]}\")\n",
        "    elif args.mode == 'tune':\n",
        "        start = timeit.default_timer()\n",
        "        best_params, best_metric = tune_hyper_parameter(\n",
        "            args.dataset, args.target_metric, device)\n",
        "        stop = timeit.default_timer()\n",
        "        run_time = stop - start\n",
        "        print()\n",
        "        print(f\"Best {args.target_metric}: {best_metric:.4f}\")\n",
        "        print(f\"Best params:\\n{best_params}\")\n",
        "        print(f\"runtime of tune_hyper_parameter: {run_time}\")\n",
        "    else:\n",
        "        raise AssertionError(f'invalid mode: {args.mode}')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "2VU_UxsGo9Gk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}