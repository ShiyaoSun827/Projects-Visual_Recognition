{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV20mCzcgMXI"
   },
   "source": [
    "## Assignment 3: Vision Transformer and Contrastive Representation Learning\n",
    "\n",
    "In this assignment, first, you are to implement [Vision Transformer](https://arxiv.org/pdf/2010.11929v2.pdf) (ViT) for image recognition. Then, you are utilize the implemented ViT to train a model using [Contrastive Representation Learning](https://arxiv.org/pdf/1503.03832.pdf), i.e. Triplet Loss.\n",
    "\n",
    "### Submission Guidelines\n",
    "The assignment codebase is split into two files. `vit_model.py` and `assignment3.ipynb`. In `vit_model.py`, incomplete codes for ViT and embedding model are provided, while in `assignment3.ipynb`, incomplete codes to iterate [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist) and initiate training and testing are provided in blocks. You are to complete the missing codeblock (Marked with comments and #TODO signs) and train the embedding representation model.\n",
    "\n",
    "In addition to PyTorch, [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning) is needed to help with the implementation of contrastive representation learning. You are strongly encouraged to go through the [README of the repo](https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/README.md), which contains the basic usage of the package. You are also strongly encouraged to study the example notebooks provided within the repo, at your own pace.\n",
    "\n",
    "To submit this assignment for grading, you must submit a compressed zipfile as `Assignment3_{YourCCID}.zip`, with three files inside: `vit_model.py`, `assignment3.ipynb`, and your trained embedding model as `vit_embeds.pt`.\n",
    "\n",
    "### Collaboration Policy\n",
    "This must be your own work. Do not share or look at the code of other students (whether they are inside or outside the class). Do not copy the code from the Internet, other than referring [PyTorch's official tutorials](https://pytorch.org/tutorials/), or the examples provided within [pytorch-metric-learning repo](https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/examples). You can talk to others in the class about solution ideas (but detailed enough that you are verbally sharing, hearing or seeing the code). You must cite whom you talked with in the comments of your programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7KpNlP9gMXM",
    "outputId": "17feb5cf-1100-4b44-a88c-86750ae5e25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-metric-learning in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from pytorch-metric-learning) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from pytorch-metric-learning) (1.3.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from pytorch-metric-learning) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from pytorch-metric-learning) (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from scikit-learn->pytorch-metric-learning) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from scikit-learn->pytorch-metric-learning) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from scikit-learn->pytorch-metric-learning) (3.2.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from tqdm->pytorch-metric-learning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
      "Collecting faiss-gpu\n",
      "  Using cached faiss-gpu-1.7.1.post2.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.post1.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.0.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.5.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.post2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [17 lines of output]\n",
      "      D:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\setuptools\\__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Requirements should be satisfied by a PEP 517 installer.\n",
      "              If you are using pip, you can try `pip install --use-pep517`.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist.fetch_build_eggs(dist.setup_requires)\n",
      "      running egg_info\n",
      "      creating C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-pip-egg-info-jqx3cxt8\\faiss_cpu.egg-info\n",
      "      writing C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-pip-egg-info-jqx3cxt8\\faiss_cpu.egg-info\\PKG-INFO\n",
      "      writing dependency_links to C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-pip-egg-info-jqx3cxt8\\faiss_cpu.egg-info\\dependency_links.txt\n",
      "      writing top-level names to C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-pip-egg-info-jqx3cxt8\\faiss_cpu.egg-info\\top_level.txt\n",
      "      writing manifest file 'C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-pip-egg-info-jqx3cxt8\\faiss_cpu.egg-info\\SOURCES.txt'\n",
      "      error: package directory 'C:\\Users\\shiya\\AppData\\Local\\Temp\\pip-install-229dtwsg\\faiss-gpu_7a4b34c46e2a4b6f8998c549b309fd48\\faiss\\faiss\\python' does not exist\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\venvpy311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Make sure the external packages are installed in Colab\n",
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-gpu\n",
    "!pip install matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VteGdBOpgMXN"
   },
   "outputs": [],
   "source": [
    "# Necessary imports for this assignment\n",
    "# NOTE: DO NOT MODIFY UNLESS NECESSARY\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop, CenterCrop\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import miners, losses, distances\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6nEjknygMXN",
    "outputId": "bf40edd3-ed6f-4751-8822-c85bae7aa8da"
   },
   "outputs": [],
   "source": [
    "# Google Colab Patch\n",
    "use_colab = True\n",
    "'''\n",
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    # ----------------------------------------\n",
    "    dir = \"/content/drive/MyDrive/Cmput328/assignment3\"    # TODO: MODIFY THIS TO INDICATE THE PARENT FOLDER OF YOUR vit_model.py file\n",
    "    # ----------------------------------------\n",
    "    sys.path.append(dir)\n",
    "'''\n",
    "from vit_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QOiRkA1-gMXO"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# NOTE: Feel free to add & modify hyperparameters if needed\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "weight_decay = 1e-4\n",
    "lr = None       # TODO: Use appropriate LR with LR scheduler\n",
    "\n",
    "# ViT specifics\n",
    "image_size = 28\n",
    "in_channels = 1\n",
    "patch_size = 4\n",
    "hidden_size = 64\n",
    "layers = 6\n",
    "heads = 8\n",
    "embed_size = 64\n",
    "\n",
    "# Contrastive Learning specifics\n",
    "margin = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziPwjCbDgMXO"
   },
   "source": [
    "### Part 1: Data Augmentation\n",
    "\n",
    "For the dataset, we will use the FashionMNIST dataset. For implmentation, you are explore the usage of different [data augmentation](https://pytorch.org/vision/master/auto_examples/transforms/plot_transforms_illustrations.html) techniques.\n",
    "\n",
    "**(1 out of 8)** You are to use at least 2 different types of data augmentation techniques (e.g. RandomHorizontalFlip) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L_l9dX5PgMXO"
   },
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation\n",
    "\n",
    "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "num_classes = len(classes)\n",
    "\n",
    "tfm_train = Compose([\n",
    "    # TODO: Add your data augmentation HERE\n",
    "    # ####################\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomRotation(degrees=10),  # Rotating the image by a maximum of 10 degrees\n",
    "\n",
    "\n",
    "\n",
    "    # ####################\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, ), (0.5, )),\n",
    "    ])\n",
    "\n",
    "tfm_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, ), (0.5, )),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "XoqLgr07gMXP",
    "outputId": "2b56c5e0-0395-4ef6-e938-ea7044fb5cb4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: Inspect your data augmentation\n",
    "def inverse_transform(\n",
    "    img_tensor: torch.Tensor,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Given a preprocessed image tensor, revert the normalization process and\n",
    "    convert the tensor back to a numpy image.\n",
    "    \"\"\"\n",
    "    inv_normalize = Normalize(mean=(-0.5/0.5, ), std=(1/0.5, ))\n",
    "    img_tensor = inv_normalize(img_tensor).permute(1, 2, 0)\n",
    "    img = np.uint8(255 * img_tensor.numpy())\n",
    "    return img\n",
    "\n",
    "# Get some random training images\n",
    "n_imgs = 5\n",
    "dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "indices = np.random.randint(0, len(dataset), size=(n_imgs, ))\n",
    "\n",
    "# Visualize with matplotlib\n",
    "for i, idx in enumerate(indices):\n",
    "    img_tensor, label = dataset[idx]\n",
    "    img = inverse_transform(img_tensor)\n",
    "    plt.subplot(1, n_imgs, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(classes[label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzqZ7PhKgMXP"
   },
   "source": [
    "### Part 2: Vision Transformer\n",
    "**(2 out of 8)** For Part 1, you are to finish the implmentation of ViT, following the discussed lab instructions. Incomplete codes are provided in file `vit_model.py`.\n",
    "\n",
    "A `check_vit.py` test script is provided to inspect the correctness of your implementations. Make sure not to modify any code in the provided test script.\n",
    "\n",
    "Additionally, the code to train a classifier is provided below. If your ViT implementation is correct, a trained model should reach 88% accuracy and above with no issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0-E4QTRqgMXQ"
   },
   "outputs": [],
   "source": [
    "# Train a Classifier\n",
    "# -----\n",
    "\n",
    "def train_classification_model():\n",
    "    # Base ViT\n",
    "    lr = 0.001  # Learning rate\n",
    "    weight_decay = 0.0001  # L2 regularization coefficient\n",
    "    num_epochs = 30\n",
    "    vit_model = ViT(\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_channels=in_channels,\n",
    "        hidden_size=hidden_size,\n",
    "        layers=layers,\n",
    "        heads=heads)\n",
    "\n",
    "    # Classifier\n",
    "    model_classifier = ClassificationHead(hidden_size=vit_model.hidden_size, num_classes=num_classes)\n",
    "    if torch.cuda.is_available():\n",
    "        vit_model = vit_model.cuda()\n",
    "        model_classifier = model_classifier.cuda()\n",
    "\n",
    "    # Use cross entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Specify optimizer\n",
    "    parameters = list(vit_model.parameters()) + list(model_classifier.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        parameters,\n",
    "        lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Evaluate at the end of each epoch\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, labels) in enumerate(train_loader):\n",
    "            vit_model.train()\n",
    "            model_classifier.train()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            feats = vit_model(x)\n",
    "            outputs = model_classifier(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # NOTE: Show train loss at the end of epoch\n",
    "            # Feel free to modify this to log more steps\n",
    "            if (i+1) % len(train_loader) == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "        # Evaluate at the end\n",
    "        test_acc = test_classification_model(vit_model, model_classifier)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            state_dict = {\n",
    "                \"classifier\": model_classifier.state_dict(),\n",
    "                \"vit\": vit_model.state_dict(),\n",
    "                \"acc\": best_acc,\n",
    "            }\n",
    "            torch.save(state_dict, \"vit_classifier.pt\")\n",
    "            print(\"Best test acc:\", best_acc)\n",
    "        print()\n",
    "\n",
    "def test_classification_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_classifier: nn.Module,\n",
    "    ):\n",
    "    # Test the model\n",
    "    vit_model.eval()\n",
    "    model_classifier.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            feats = vit_model(images)\n",
    "            outputs = model_classifier(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jo7hlyuvgMXQ",
    "outputId": "12c118f3-09a6-4007-be92-3ee603fd39b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [118/118], Loss: 0.7689\n",
      "Test Accuracy: 76.34 %\n",
      "Best test acc: 0.7634\n",
      "\n",
      "Epoch [2/30], Step [118/118], Loss: 0.6753\n",
      "Test Accuracy: 80.75 %\n",
      "Best test acc: 0.8075\n",
      "\n",
      "Epoch [3/30], Step [118/118], Loss: 0.6940\n",
      "Test Accuracy: 81.1 %\n",
      "Best test acc: 0.811\n",
      "\n",
      "Epoch [4/30], Step [118/118], Loss: 0.6422\n",
      "Test Accuracy: 81.82 %\n",
      "Best test acc: 0.8182\n",
      "\n",
      "Epoch [5/30], Step [118/118], Loss: 0.4043\n",
      "Test Accuracy: 84.77 %\n",
      "Best test acc: 0.8477\n",
      "\n",
      "Epoch [6/30], Step [118/118], Loss: 0.4652\n",
      "Test Accuracy: 84.7 %\n",
      "\n",
      "Epoch [7/30], Step [118/118], Loss: 0.3894\n",
      "Test Accuracy: 85.56 %\n",
      "Best test acc: 0.8556\n",
      "\n",
      "Epoch [8/30], Step [118/118], Loss: 0.3706\n",
      "Test Accuracy: 85.48 %\n",
      "\n",
      "Epoch [9/30], Step [118/118], Loss: 0.3640\n",
      "Test Accuracy: 85.55 %\n",
      "\n",
      "Epoch [10/30], Step [118/118], Loss: 0.3478\n",
      "Test Accuracy: 86.46 %\n",
      "Best test acc: 0.8646\n",
      "\n",
      "Epoch [11/30], Step [118/118], Loss: 0.4394\n",
      "Test Accuracy: 86.67 %\n",
      "Best test acc: 0.8667\n",
      "\n",
      "Epoch [12/30], Step [118/118], Loss: 0.3080\n",
      "Test Accuracy: 86.16 %\n",
      "\n",
      "Epoch [13/30], Step [118/118], Loss: 0.4732\n",
      "Test Accuracy: 87.05 %\n",
      "Best test acc: 0.8705\n",
      "\n",
      "Epoch [14/30], Step [118/118], Loss: 0.3291\n",
      "Test Accuracy: 87.21 %\n",
      "Best test acc: 0.8721\n",
      "\n",
      "Epoch [15/30], Step [118/118], Loss: 0.4125\n",
      "Test Accuracy: 87.48 %\n",
      "Best test acc: 0.8748\n",
      "\n",
      "Epoch [16/30], Step [118/118], Loss: 0.2986\n",
      "Test Accuracy: 87.45 %\n",
      "\n",
      "Epoch [17/30], Step [118/118], Loss: 0.2211\n",
      "Test Accuracy: 87.87 %\n",
      "Best test acc: 0.8787\n",
      "\n",
      "Epoch [18/30], Step [118/118], Loss: 0.4744\n",
      "Test Accuracy: 88.25 %\n",
      "Best test acc: 0.8825\n",
      "\n",
      "Epoch [19/30], Step [118/118], Loss: 0.2378\n",
      "Test Accuracy: 88.26 %\n",
      "Best test acc: 0.8826\n",
      "\n",
      "Epoch [20/30], Step [118/118], Loss: 0.2871\n",
      "Test Accuracy: 88.62 %\n",
      "Best test acc: 0.8862\n",
      "\n",
      "Epoch [21/30], Step [118/118], Loss: 0.3361\n",
      "Test Accuracy: 88.54 %\n",
      "\n",
      "Epoch [22/30], Step [118/118], Loss: 0.3209\n",
      "Test Accuracy: 88.59 %\n",
      "\n",
      "Epoch [23/30], Step [118/118], Loss: 0.2960\n",
      "Test Accuracy: 88.53 %\n",
      "\n",
      "Epoch [24/30], Step [118/118], Loss: 0.2305\n",
      "Test Accuracy: 88.76 %\n",
      "Best test acc: 0.8876\n",
      "\n",
      "Epoch [25/30], Step [118/118], Loss: 0.1896\n",
      "Test Accuracy: 88.59 %\n",
      "\n",
      "Epoch [26/30], Step [118/118], Loss: 0.1952\n",
      "Test Accuracy: 88.72 %\n",
      "\n",
      "Epoch [27/30], Step [118/118], Loss: 0.2083\n",
      "Test Accuracy: 89.01 %\n",
      "Best test acc: 0.8901\n",
      "\n",
      "Epoch [28/30], Step [118/118], Loss: 0.3447\n",
      "Test Accuracy: 89.17 %\n",
      "Best test acc: 0.8917\n",
      "\n",
      "Epoch [29/30], Step [118/118], Loss: 0.2439\n",
      "Test Accuracy: 88.78 %\n",
      "\n",
      "Epoch [30/30], Step [118/118], Loss: 0.2927\n",
      "Test Accuracy: 89.05 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Uncomment this to test your ViT implementation\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "train_classification_model()\n",
    "\n",
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLhRJHSdgMXQ"
   },
   "source": [
    "### Part 3: Contrastive Representation Learning\n",
    "In this part, you are to use [PyTorch-Metric-Learning](https://github.com/KevinMusgrave/pytorch-metric-learning) to implement triplet loss and learn an embedding representation model with ViT. Quick tutorials and notebooks are available in the repo.\n",
    "\n",
    "However, only a subset of data from the FashionMNIST training set (around 1/12) will be used to train the model. The code to subsample the full train dataset is provided in `sample_balanced_subset`. You must not modfiy the code in that block.\n",
    "\n",
    "To pass this part, you are to reach **60%** and above in Precision@1 metric scoring on the full FashionMNIST test dataset with your trained embedding model. The final model must be submitted along with your notebook with logging outputs. The test function is provided in `evaluate_at_end_epoch`.\n",
    "\n",
    "Furthermore, you are required to employ one of the learning rate schedulers, like [MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR), [Consine Annealing LR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR). Or you can implement your own learning rate adjusting strategy, as a function to training iteration or epoch, using [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR). The marking is distributed as follows:\n",
    "\n",
    "- (2 out of 8) Complete `train_embedding_model`. Add your selected learning rate scheduler, and train your embedding model\n",
    "- (0.5 out of 8) Complete `get_embeddings` to correctly perform inference with your embedding model.\n",
    "- (1 out of 8) Attach the notebook with full training logs (do not clear your notebook outputs when finished running), and your trained model file with **60%** and above in Precision@1 metric scoring, as `vit_embeds.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aaane5evgMXQ",
    "outputId": "19eee3fc-ff9b-4450-c603-0ff72213e787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. samples in subset: 5000\n"
     ]
    }
   ],
   "source": [
    "# Sample a subset of training dataset\n",
    "# NOTE: sample an even number of samples per class\n",
    "# To migrate the problem of [class imbalancing]\n",
    "# (https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data).\n",
    "def sample_balanced_subset(\n",
    "    train_dataset,\n",
    "    n_samples_per_class: int = 50,\n",
    "    n_classes: int = 10\n",
    "    ):\n",
    "    maps = {i: [] for i in range(n_classes)}\n",
    "    for idx, (_, cls_idx) in enumerate(train_dataset):\n",
    "        if len(maps[cls_idx]) < n_samples_per_class:\n",
    "            maps[cls_idx].append(idx)\n",
    "\n",
    "    indices = []\n",
    "    for _, ind in maps.items():\n",
    "        indices += ind\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "    print(\"Num. samples in subset:\", len(train_subset))\n",
    "    return train_subset\n",
    "\n",
    "# Subsample a small subset from the full train dataset\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
    "train_dataset = sample_balanced_subset(full_train_dataset, n_samples_per_class=500)      # DO NOT MODIFY THIS CODE\n",
    "\n",
    "# Full test set will still be used for testing\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1k2A_4OygMXR"
   },
   "outputs": [],
   "source": [
    "# Train an embedding model\n",
    "# -----\n",
    "import torch.optim as optim\n",
    "def train_embedding_model(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Evaluate model per epoch, and keep track of the model performance at that time\n",
    "    best_prec = 0.0\n",
    "\n",
    "    # Triplet Loss with Semi-hard triples & l2 distance\n",
    "    # -------------------\n",
    "    # TODO: Specify distance, objective function (triplet loss), and your miner to sample triplet pairs\n",
    "\n",
    "    distance = distances.LpDistance(power=2)\n",
    "    criterion = losses.TripletMarginLoss(margin=0.2, distance=distance)\n",
    "    miner = miners.BatchHardMiner(distance=distance)#miners.DistanceWeightedMiner(distance=distance)#miners.SemiHardNegativeTripletMiner(distance=distance)#miners.BatchSemiHardMiner(distance=distance, type_of_triplets=\"semi-hard\")        # Make sure to use \"semihard\" triplets\n",
    "    #criterion = losses.TripletMarginLoss(margin=0.2, distance=distance)\n",
    "    #miner = miners.SemiHardNegativeTripletMiner(distance=distance)        # \"semihard\" triplets\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # Precision to test embedding quality\n",
    "    accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "\n",
    "    # Models to train\n",
    "    # -------------------\n",
    "    # TODO: Specify your model(s) correctly here\n",
    "    # Don't forget to send them to cuda/gpu\n",
    "    vit_model = ViT(\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_channels=in_channels,\n",
    "        hidden_size=hidden_size,\n",
    "        layers=layers,\n",
    "        heads=heads\n",
    "    ).cuda()\n",
    "    #model_embedding = nn.Sequential(nn.Linear(vit_model.hidden_size, embedding_size), nn.ReLU(), nn.Linear(embedding_size, embedding_size))\n",
    "    #embedding_dim = 64\n",
    "    #model_embedding = nn.Sequential(\n",
    "     #   nn.Linear(vit_model.hidden_size, embedding_dim),\n",
    "     #   nn.ReLU()\n",
    "    #).cuda()\n",
    "    model_embedding = LinearEmbeddingHead(hidden_size=hidden_size,embed_size=64).cuda()\n",
    "    # -------------------\n",
    "\n",
    "    # Specify optimizer\n",
    "    # -------------------\n",
    "    #parameters = None       # TODO: correctly specify the trainable parameters here\n",
    "\n",
    "    #optimizer = None        # TODO: Specify your optimizer\n",
    "\n",
    "    #scheduler = None        # TODO: Specify your LR scheduler\n",
    "    #parameters = list(vit_model.parameters()) + list(model_embedding.parameters())\n",
    "    #optimizer = optim.AdamW(parameters, lr=0.001, weight_decay=0.01)\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "    parameters = list(vit_model.parameters()) + list(model_embedding.parameters())\n",
    "    #optimizer = torch.optim.AdamW(parameters, lr=0.0001, weight_decay=0.0001)\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1,5], gamma=0.1)  # Example scheduler\n",
    "    #lr = 0.001,[5,10]\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=10)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, labels) in enumerate(train_loader):\n",
    "            vit_model.train()\n",
    "            model_embedding.train()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            # -------------------\n",
    "            # TODO: Implement forward and backward pass\n",
    "            # TODO: Correctly update your LR with your selected LR scheduler\n",
    "            feats = vit_model(x)\n",
    "            embeddings = model_embedding(feats)\n",
    "\n",
    "            hard_triplets = miner(embeddings, labels)\n",
    "            loss = criterion(embeddings, labels, hard_triplets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # -------------------\n",
    "\n",
    "            # NOTE: Show train loss at the end of epoch\n",
    "            # Feel free to modify this to log more steps\n",
    "            if (i+1) % len(train_loader) == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "        # Evaluate at the end\n",
    "        best_prec = evaluate_at_end_epoch(vit_model, model_embedding, accuracy_calculator, best_prec, train_dataset, test_dataset)\n",
    "\n",
    "    # #########################\n",
    "\n",
    "def get_embeddings(\n",
    "    x: torch.Tensor,\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Calculate embeddings for a batch of images.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "    # TODO: Correctly calculate x_embeds for a batch of\n",
    "    # images x with shape (batch_size, 3, h, w)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vit_model.eval()\n",
    "        model_embedding.eval()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "\n",
    "        feats = vit_model(x)\n",
    "        x_embeds = model_embedding(feats)\n",
    "\n",
    "\n",
    "    #########################\n",
    "\n",
    "    x_embeds = x_embeds.cpu()   # Cast to CPU\n",
    "    x_embeds = torch.nn.functional.normalize(x_embeds, p=2, dim=1)      # Extra Step: Normalize the embeddings\n",
    "    return x_embeds\n",
    "\n",
    "def get_embeddings_over_dataset(\n",
    "    dataset,\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    ):\n",
    "    \"\"\"Loop through a full dataset and return all embeddings.\n",
    "    \"\"\"\n",
    "    # Create a loader on the go\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    X_embeds, Y = [], []\n",
    "    for i, (x, y) in enumerate(tqdm.tqdm(loader)):\n",
    "        x_embeds = get_embeddings(x, vit_model, model_embedding)\n",
    "        X_embeds.append(x_embeds)\n",
    "        Y.append(y)\n",
    "\n",
    "    X_embeds = torch.cat(X_embeds, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "    return X_embeds, Y\n",
    "\n",
    "def test_embedding_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    accuracy_calculator,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    # Test the model\n",
    "    model_embedding.eval()\n",
    "\n",
    "    X_embeds, Y = get_embeddings_over_dataset(train_dataset, vit_model, model_embedding)\n",
    "    X_embeds_test, Y_test = get_embeddings_over_dataset(test_dataset, vit_model, model_embedding)\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        X_embeds_test, Y_test, X_embeds, Y, False\n",
    "    )\n",
    "    return accuracies[\"precision_at_1\"]\n",
    "\n",
    "def evaluate_at_end_epoch(\n",
    "    vit_model: nn.Module,\n",
    "    model_embedding: nn.Module,\n",
    "    accuracy_calculator,\n",
    "    best_prec: float,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    ):\n",
    "    # Evaluate at the end\n",
    "    prec = test_embedding_model(vit_model, model_embedding, accuracy_calculator, train_dataset, test_dataset)\n",
    "    if prec > best_prec:\n",
    "        best_prec = prec\n",
    "        state_dict = {\n",
    "            \"embedding_head\": model_embedding.state_dict(),\n",
    "            \"vit\": vit_model.state_dict(),\n",
    "            \"precision@1\": prec,\n",
    "        }\n",
    "        torch.save(state_dict, \"vit_embeds.pt\")\n",
    "        print(\"Best Precision@1:\", best_prec)\n",
    "    print()\n",
    "    return best_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE_Ym955V5Rw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHn-ESZ3gMXR",
    "outputId": "8683f4e5-84c2-4faf-be76-593fbc6218f2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Ualberta\\Cmput328\\Assignment3\\assignment3.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO: Uncomment this to train your embedding model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()    \u001b[39m# Clean-up memory 1st\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_embedding_model(train_dataset, test_dataset)\n",
      "\u001b[1;32mf:\\Ualberta\\Cmput328\\Assignment3\\assignment3.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m miner \u001b[39m=\u001b[39m miners\u001b[39m.\u001b[39mBatchHardMiner(distance\u001b[39m=\u001b[39mdistance)\u001b[39m#miners.DistanceWeightedMiner(distance=distance)#miners.SemiHardNegativeTripletMiner(distance=distance)#miners.BatchSemiHardMiner(distance=distance, type_of_triplets=\"semi-hard\")        # Make sure to use \"semihard\" triplets\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#criterion = losses.TripletMarginLoss(margin=0.2, distance=distance)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#miner = miners.SemiHardNegativeTripletMiner(distance=distance)        # \"semihard\" triplets\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# -------------------\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Precision to test embedding quality\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m accuracy_calculator \u001b[39m=\u001b[39m AccuracyCalculator(include\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision_at_1\u001b[39;49m\u001b[39m\"\u001b[39;49m,), k\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Models to train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# -------------------\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# TODO: Specify your model(s) correctly here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Don't forget to send them to cuda/gpu\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m vit_model \u001b[39m=\u001b[39m ViT(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     image_size\u001b[39m=\u001b[39mimage_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     patch_size\u001b[39m=\u001b[39mpatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     heads\u001b[39m=\u001b[39mheads\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m )\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\pytorch_metric_learning\\utils\\accuracy_calculator.py:239\u001b[0m, in \u001b[0;36mAccuracyCalculator.__init__\u001b[1;34m(self, include, exclude, avg_of_avgs, return_per_class, k, label_comparison_fn, device, knn_func, kmeans_func)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_per_class \u001b[39m=\u001b[39m return_per_class\n\u001b[0;32m    238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m c_f\u001b[39m.\u001b[39muse_cuda_if_available() \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m device\n\u001b[1;32m--> 239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mknn_func \u001b[39m=\u001b[39m FaissKNN() \u001b[39mif\u001b[39;00m knn_func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m knn_func\n\u001b[0;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkmeans_func \u001b[39m=\u001b[39m (\n\u001b[0;32m    241\u001b[0m     FaissKMeans(niter\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m kmeans_func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39melse\u001b[39;00m kmeans_func\n\u001b[0;32m    244\u001b[0m )\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(k, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m k \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mand\u001b[39;00m (k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mmax_bin_count\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\pytorch_metric_learning\\utils\\inference.py:160\u001b[0m, in \u001b[0;36mFaissKNN.__init__\u001b[1;34m(self, reset_before, reset_after, index_init_fn, gpus)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_before \u001b[39m=\u001b[39m reset_before\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_after \u001b[39m=\u001b[39m reset_after\n\u001b[0;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_init_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m     faiss\u001b[39m.\u001b[39mIndexFlatL2 \u001b[39mif\u001b[39;00m index_init_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m index_init_fn\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m gpus \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(gpus, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Uncomment this to train your embedding model\n",
    "torch.cuda.empty_cache()    # Clean-up memory 1st\n",
    "train_embedding_model(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2UlAmrogMXR"
   },
   "source": [
    "### Part 4: Application\n",
    "In the below examples, we perform inference with the trained embedding model using K-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpyL7KRqgMXR",
    "outputId": "3f7038d2-a2b3-4d76-9d64-31e05bc16628"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vit_embeds.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Ualberta\\Cmput328\\Assignment3\\assignment3.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     model_embedding \u001b[39m=\u001b[39m model_embedding\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Load saved checkpoint\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mvit_embeds.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m vit_model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mvit\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Ualberta/Cmput328/Assignment3/assignment3.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model_embedding\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39membedding_head\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\venvpy311\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vit_embeds.pt'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()    # Clean-up memory 1st\n",
    "\n",
    "# Load your trained model\n",
    "# Base ViT\n",
    "vit_model = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_channels=in_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    layers=layers,\n",
    "    heads=heads)\n",
    "\n",
    "# Embedding head\n",
    "model_embedding = LinearEmbeddingHead(hidden_size=vit_model.hidden_size, embed_size=embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    vit_model = vit_model.cuda()\n",
    "    model_embedding = model_embedding.cuda()\n",
    "\n",
    "# Load saved checkpoint\n",
    "checkpoint = torch.load(\"vit_embeds.pt\")\n",
    "\n",
    "vit_model.load_state_dict(checkpoint[\"vit\"])\n",
    "\n",
    "model_embedding.load_state_dict(checkpoint[\"embedding_head\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa_avwBWgMXR"
   },
   "source": [
    "To quickly infer K-nearest neighbors, we construct a small bank, where the (image, label) pairs in the bank are sampled from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExSXAfkegMXR",
    "outputId": "1094f80c-4cb5-477b-ca43-5d81e9621897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. samples in subset: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample a bank, which is a small subset of trainset\n",
    "bank = sample_balanced_subset(train_dataset, n_samples_per_class=25)\n",
    "X_bank_embeds, Y_bank = get_embeddings_over_dataset(bank, vit_model=vit_model, model_embedding=model_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAH2zpRngMXS"
   },
   "source": [
    "Now, we perform the K-nearest neighbors, by comparing our sample against the embeddings in the bank.\n",
    "\n",
    "**(0.5 out of 8)** You are to finish the implementation of the function `retrieve_topk_nearest_neighbors_l2`, in order to acquire the demoing visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "kbnQ_nGZgMXS",
    "outputId": "c9733791-5ade-4c80-d5a5-7767836ce21c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb127dac0d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deep KNN\n",
    "def retrieve_topk_nearest_neighbors_l2(\n",
    "    query: torch.Tensor,\n",
    "    ref: torch.Tensor,\n",
    "    k: int = 5,\n",
    "    ):\n",
    "    \"\"\"Calculate L2 distance between query and ref embeddings.\n",
    "    Return the top-k scores and the indices of the top-k scores.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "    # TODO: Implement L2 distance and topk ops\n",
    "    #distances = torch.norm(query - ref, dim=1)\n",
    "    difference = torch.subtract(query, ref)\n",
    "    distances = torch.norm(difference, dim=1)\n",
    "\n",
    "    top_k_scores, indices = torch.topk(distances, k=k)\n",
    "\n",
    "\n",
    "\n",
    "    # #########################\n",
    "\n",
    "    return top_k_scores, indices\n",
    "\n",
    "# Grab some samples in test set\n",
    "dataiter = iter(test_loader)\n",
    "X, Y = next(dataiter)\n",
    "X_embeds = get_embeddings(X, vit_model=vit_model, model_embedding=model_embedding)\n",
    "\n",
    "# One random sample\n",
    "idx = np.random.randint(0, X.shape[0])\n",
    "img, label, embed = X[idx], Y[idx], X_embeds[idx]\n",
    "\n",
    "# Get top-k nearest samples\n",
    "values, indices = retrieve_topk_nearest_neighbors_l2(embed.unsqueeze(0), X_bank_embeds)\n",
    "\n",
    "# Visualize results\n",
    "_, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
    "axes[0].imshow(inverse_transform(img), cmap=\"gray\")\n",
    "axes[0].set_title(classes[label])\n",
    "for i, ind in enumerate(indices):\n",
    "    img_bank, label_bank = bank[ind]\n",
    "    axes[i + 1].imshow(inverse_transform(img_bank), cmap=\"gray\")\n",
    "    axes[i + 1].set_title(\"{}\\nScore: {:.3f}\".format(classes[label_bank], values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmsgSXkpgMXS"
   },
   "source": [
    "The trained embedding model can also be used to train a new classifier.\n",
    "\n",
    "The intuition behind **Transfer Learning**, and a referring tutorial can be seen [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n",
    "\n",
    "**(1 out of 8)** Complete your code to train a classification head only, using your trained ViT embedding model. But you should not modify your trained ViT model weights in your `train_classification_model_head_only` function.\n",
    "\n",
    "A sample function to test with the classification head is provided, as `test_classification_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "Y_4yK0h6gMXS"
   },
   "outputs": [],
   "source": [
    "# Train a Classifier\n",
    "# -----\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_classification_model_head_only(\n",
    "    vit_model: nn.Module,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    num_epochs: int = 10,\n",
    "    ):\n",
    "    #########################\n",
    "    # Finish Your Code HERE\n",
    "    # #########################\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    #test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "    num_classes = 25\n",
    "    # Classifier\n",
    "    #model_classifier = nn.Linear(vit_model.hidden_size, num_classes).cuda() if torch.cuda.is_available() else nn.Linear(vit_model.hidden_size, num_classes)\n",
    "    #model_classifier=KNeighborsClassifier(n_neighbors = k)\n",
    "    vit_model = vit_model\n",
    "\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.Adam(model_classifier.parameters(), lr=0.001)\n",
    "    #best_acc = 0.0\n",
    "    #model_classifier = nn.Linear(vit_model.hidden_size, num_classes).cuda() if torch.cuda.is_available() else nn.Linear(vit_model.hidden_size, num_classes)\n",
    "    model_classifier = nn.Linear(vit_model.hidden_size, num_classes).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model_classifier.parameters(), lr=0.001)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Freeze the weights of vit_model during training\n",
    "    for param in vit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            features = vit_model(images)\n",
    "            outputs = model_classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute accuracy on the test dataset after every epoch\n",
    "        acc = test_classification_model(vit_model, model_classifier)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #########################\n",
    "\n",
    "    # You should return the weights of your trained model, and the classification score (accuracy)\n",
    "    return {\"acc\": acc, \"vit\": vit_model.state_dict(), \"classifier\":model_classifier.state_dict()}\n",
    "\n",
    "def test_classification_model(\n",
    "    vit_model: nn.Module,\n",
    "    model_classifier: nn.Module,\n",
    "    ):\n",
    "    # Test the model\n",
    "    vit_model.eval()\n",
    "    model_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            feats = vit_model(images)\n",
    "            outputs = model_classifier(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sa_LoIcUgMXS",
    "outputId": "d51e4829-3260-455b-c055-dff0a3bacdf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.0 %\n",
      "Test Accuracy: 10.08 %\n",
      "Test Accuracy: 14.68 %\n",
      "Test Accuracy: 10.85 %\n",
      "Test Accuracy: 12.76 %\n",
      "Test Accuracy: 17.49 %\n",
      "Test Accuracy: 10.0 %\n",
      "Test Accuracy: 16.92 %\n",
      "Test Accuracy: 17.29 %\n",
      "Test Accuracy: 10.24 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.1024,\n",
       " 'vit': OrderedDict([('patch_embed.projection.weight',\n",
       "               tensor([[[[-0.1835, -0.1979, -0.0795,  0.1583],\n",
       "                         [ 0.2314, -0.1939, -0.1575,  0.1412],\n",
       "                         [-0.0342, -0.0938,  0.0675,  0.0856],\n",
       "                         [ 0.1280, -0.0883,  0.0431,  0.0818]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1072,  0.0541,  0.1320, -0.1355],\n",
       "                         [-0.1904, -0.1812,  0.1187, -0.0236],\n",
       "                         [ 0.0215,  0.0939,  0.0422,  0.1958],\n",
       "                         [-0.1774, -0.1203,  0.0632, -0.0544]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1177, -0.1040,  0.2073,  0.1897],\n",
       "                         [-0.1212, -0.0025,  0.1663, -0.0704],\n",
       "                         [ 0.1314, -0.0573,  0.2379, -0.2178],\n",
       "                         [-0.0571, -0.0598,  0.1324, -0.1551]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0826, -0.1542, -0.0842,  0.1508],\n",
       "                         [-0.0082, -0.0870, -0.0882,  0.0439],\n",
       "                         [-0.1473,  0.2156,  0.0993, -0.0300],\n",
       "                         [ 0.0126, -0.1143, -0.1140, -0.1307]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.2158,  0.2061,  0.0651, -0.0713],\n",
       "                         [ 0.1342, -0.2446, -0.1988, -0.0520],\n",
       "                         [-0.1983,  0.0124, -0.0669, -0.2547],\n",
       "                         [ 0.2042, -0.1536, -0.2311,  0.1739]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0349,  0.0130,  0.1379, -0.0983],\n",
       "                         [ 0.0387, -0.1179,  0.1467,  0.0354],\n",
       "                         [ 0.2432, -0.1942,  0.2550,  0.0268],\n",
       "                         [-0.1489,  0.1278, -0.2415,  0.0145]]]], device='cuda:0')),\n",
       "              ('patch_embed.projection.bias',\n",
       "               tensor([ 0.0363, -0.0786,  0.0144, -0.0597,  0.0358,  0.2259, -0.1111,  0.1869,\n",
       "                       -0.0387, -0.0909, -0.2059, -0.0416,  0.1255, -0.1300,  0.1092, -0.2368,\n",
       "                       -0.2091, -0.0794,  0.1549, -0.1603,  0.1446, -0.0145, -0.1637, -0.0494,\n",
       "                        0.1029,  0.0069, -0.1598, -0.2212,  0.1862,  0.1325,  0.1602,  0.1655,\n",
       "                       -0.0798, -0.1751, -0.2141,  0.2466, -0.1572, -0.2168,  0.2461, -0.1419,\n",
       "                       -0.0583, -0.2348,  0.0671,  0.2124, -0.1277,  0.0657, -0.2344, -0.1992,\n",
       "                       -0.2121, -0.1317,  0.0629,  0.2028, -0.2142, -0.1401, -0.1415, -0.0108,\n",
       "                        0.1304,  0.1998, -0.1073,  0.2134, -0.0733, -0.0399,  0.1106, -0.0639],\n",
       "                      device='cuda:0')),\n",
       "              ('pos_embed.cls_token',\n",
       "               tensor([[[ 1.9022,  0.2766, -0.3737,  0.4132, -1.4403, -0.8272, -0.9985,\n",
       "                         -1.1735, -0.1277,  0.0887, -0.7073,  1.7828,  0.9149, -0.2288,\n",
       "                         -0.1519,  0.8284, -0.3297,  0.1702, -0.2142, -0.7755,  0.8044,\n",
       "                          0.3737, -0.0824,  2.1731,  0.3216, -0.1184,  1.3294,  1.4925,\n",
       "                          0.4961,  0.7693, -0.4735,  0.5103,  1.1103,  0.2312,  1.9319,\n",
       "                         -1.4681,  0.0257, -0.0781, -1.4811,  1.4504,  0.1432,  0.4278,\n",
       "                         -1.3196,  1.1728, -1.8640,  0.4746, -0.2616, -0.0550,  2.2368,\n",
       "                         -0.7026,  0.3368, -0.8250,  0.1467,  0.7801,  0.7194, -1.5892,\n",
       "                          0.4722,  0.2811,  1.3273,  0.6741, -0.4515,  0.1065, -1.3276,\n",
       "                          0.2638]]], device='cuda:0')),\n",
       "              ('pos_embed.position_embeddings',\n",
       "               tensor([[[ 0.1037,  1.4081,  0.6335,  ...,  0.8489, -0.2803, -0.1410],\n",
       "                        [-1.1499,  0.2060,  0.7884,  ...,  1.2042, -0.0300, -1.8539],\n",
       "                        [ 1.7211,  0.0723, -0.2875,  ...,  1.3584, -0.8762,  1.3426],\n",
       "                        ...,\n",
       "                        [ 1.2599,  0.4502,  0.3127,  ...,  0.0585, -0.8672, -0.1303],\n",
       "                        [-0.2806,  1.0817,  0.8122,  ...,  0.9109, -0.4020,  1.4897],\n",
       "                        [ 1.0402,  0.6607,  0.2697,  ..., -0.5524, -0.5875,  0.6804]]],\n",
       "                      device='cuda:0')),\n",
       "              ('ln_pre.weight',\n",
       "               tensor([0.9976, 0.9923, 0.9997, 1.0031, 1.0028, 0.9867, 0.9940, 1.0046, 1.0040,\n",
       "                       1.0018, 1.0031, 0.9924, 0.9903, 1.0016, 0.9953, 1.0026, 1.0015, 0.9975,\n",
       "                       0.9985, 1.0061, 0.9883, 1.0036, 1.0002, 0.9912, 0.9933, 1.0031, 0.9973,\n",
       "                       0.9921, 0.9944, 0.9963, 1.0013, 0.9985, 1.0015, 0.9949, 0.9973, 0.9938,\n",
       "                       1.0045, 0.9956, 1.0041, 0.9955, 0.9861, 1.0046, 1.0029, 0.9885, 0.9903,\n",
       "                       0.9926, 0.9959, 0.9945, 0.9925, 1.0056, 0.9939, 0.9905, 1.0038, 0.9956,\n",
       "                       1.0021, 0.9922, 0.9892, 0.9897, 1.0016, 0.9944, 0.9986, 1.0015, 1.0038,\n",
       "                       1.0013], device='cuda:0')),\n",
       "              ('ln_pre.bias',\n",
       "               tensor([ 5.2200e-03, -4.6529e-03, -4.8207e-03,  2.4918e-03,  5.3580e-03,\n",
       "                       -5.1869e-03, -5.5666e-03, -4.2001e-03, -5.3389e-03,  3.5551e-03,\n",
       "                       -7.4640e-03, -2.2431e-03,  4.7122e-03,  4.0146e-03, -3.7867e-03,\n",
       "                       -5.9054e-03, -5.0134e-03, -5.0264e-03,  6.6231e-03,  1.0011e-02,\n",
       "                        9.3426e-03,  1.0315e-03, -5.7530e-03, -5.8152e-03, -3.8199e-03,\n",
       "                        5.2185e-03,  4.8083e-03, -4.7818e-03, -5.4960e-05,  5.6682e-03,\n",
       "                       -5.2078e-03,  5.0055e-03,  1.8199e-03,  4.5887e-03,  4.6038e-03,\n",
       "                        4.8020e-03, -5.0591e-03,  2.6359e-03, -4.2856e-03,  5.2604e-03,\n",
       "                        5.7016e-03, -5.3462e-03, -4.8018e-03, -6.0144e-03,  4.6716e-03,\n",
       "                        5.2898e-03,  4.3607e-03,  4.7571e-03,  4.4282e-03,  4.0292e-03,\n",
       "                        5.0672e-03,  6.3156e-03, -5.2561e-03, -6.3249e-03,  5.8961e-03,\n",
       "                        4.6039e-03, -4.3397e-03,  4.6885e-03, -4.1825e-03, -3.0172e-03,\n",
       "                       -6.5370e-03,  5.2468e-03,  4.3838e-03, -4.7709e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.0.attn.in_proj_weight',\n",
       "               tensor([[-9.8462e-02, -3.8197e-02,  3.2234e-02,  ..., -1.1462e-01,\n",
       "                         7.3676e-02, -7.6617e-02],\n",
       "                       [ 1.0374e-01, -1.2969e-01, -7.3701e-02,  ..., -8.0553e-05,\n",
       "                        -9.5460e-02, -1.3120e-01],\n",
       "                       [-1.1371e-02,  1.7598e-02,  4.1253e-02,  ...,  1.4732e-01,\n",
       "                        -1.3338e-02, -3.6210e-02],\n",
       "                       ...,\n",
       "                       [-5.1056e-02, -1.4770e-01,  1.5783e-01,  ...,  3.5203e-02,\n",
       "                        -5.7973e-02, -8.9985e-02],\n",
       "                       [-1.2846e-01,  1.2042e-01,  1.0032e-01,  ..., -1.2840e-01,\n",
       "                        -6.1833e-02, -5.0154e-02],\n",
       "                       [ 4.4544e-02, -7.3414e-02,  1.4496e-01,  ...,  9.3324e-02,\n",
       "                        -5.2633e-02,  1.1319e-01]], device='cuda:0')),\n",
       "              ('transformer.resblocks.0.attn.in_proj_bias',\n",
       "               tensor([-5.5874e-03,  4.9849e-03, -7.7198e-03,  7.4884e-03, -6.7987e-03,\n",
       "                        9.6329e-03, -4.7292e-03, -5.0938e-03, -3.7321e-03, -4.5982e-03,\n",
       "                        6.8625e-03,  5.1968e-03, -4.6120e-03,  4.9031e-03,  4.2612e-03,\n",
       "                       -5.2338e-03, -4.5766e-03, -8.1796e-03,  6.9189e-03,  6.3127e-03,\n",
       "                        4.8322e-03,  5.3618e-03,  1.8233e-03,  4.9181e-03, -4.8426e-03,\n",
       "                       -2.7727e-04, -5.1422e-03, -1.9168e-03, -5.5294e-03, -5.6829e-03,\n",
       "                        5.2907e-03,  4.8146e-03, -5.1362e-03, -4.1631e-03,  8.2144e-03,\n",
       "                        5.1662e-03, -4.9289e-03, -5.7785e-03, -5.8484e-03,  5.3633e-03,\n",
       "                        4.5985e-03, -3.9074e-03,  5.4901e-03,  4.8284e-03, -2.6437e-03,\n",
       "                        7.7019e-03,  4.8908e-03,  3.1511e-03,  5.3471e-03, -5.2813e-03,\n",
       "                       -5.2438e-04,  5.2248e-03,  6.2256e-03,  5.3298e-03, -7.8891e-03,\n",
       "                        1.0210e-02,  4.7170e-03,  4.9562e-03, -5.3297e-03, -7.1432e-03,\n",
       "                        5.2641e-03,  5.1298e-03, -4.0054e-03, -4.6713e-03, -3.2856e-05,\n",
       "                        1.2752e-05,  8.2025e-06,  1.1409e-06, -4.0194e-05,  1.6870e-05,\n",
       "                        1.2355e-05,  5.4063e-06, -8.6120e-06, -5.1994e-06,  1.3349e-05,\n",
       "                       -1.0160e-05,  6.7376e-07, -4.6552e-05,  2.1160e-06, -1.7158e-05,\n",
       "                       -1.6026e-05,  3.5588e-06,  1.5719e-05,  9.7542e-06,  2.3143e-05,\n",
       "                        1.6126e-05,  3.9235e-06,  2.9643e-05,  2.6707e-06,  4.0685e-06,\n",
       "                       -5.6088e-06, -3.3104e-06,  4.5763e-06,  5.3279e-06, -1.1068e-06,\n",
       "                        1.9499e-07,  2.2058e-07, -3.4267e-06,  1.1864e-05,  2.4720e-07,\n",
       "                        1.3242e-05,  1.2180e-05, -7.5947e-06,  6.5260e-06,  6.1498e-06,\n",
       "                       -4.9425e-06,  7.2206e-06, -8.9805e-06,  4.6573e-06,  1.2555e-05,\n",
       "                        4.7190e-06,  2.4734e-06,  1.1894e-05,  3.0234e-06,  2.8550e-06,\n",
       "                       -2.2158e-05,  1.2509e-06,  9.4143e-06, -8.8163e-06, -3.6665e-06,\n",
       "                        2.7741e-06,  1.8254e-05,  7.8047e-06, -3.4787e-07, -1.2865e-05,\n",
       "                        1.4964e-05,  4.2946e-07,  2.0782e-06, -5.6633e-03, -4.5313e-03,\n",
       "                        4.9043e-03,  4.7977e-03,  5.3896e-03, -4.9874e-03, -5.5263e-03,\n",
       "                       -5.9583e-03, -4.1071e-03,  5.6824e-03,  4.4640e-03,  4.0877e-03,\n",
       "                       -5.2287e-03,  1.6295e-03, -3.7278e-03,  7.8097e-03, -4.6949e-03,\n",
       "                        4.8523e-03, -6.1479e-03,  3.4756e-03,  4.8571e-03,  4.9236e-03,\n",
       "                        4.9933e-03,  4.4526e-03,  4.9877e-03, -5.0049e-03,  4.7298e-03,\n",
       "                       -4.4435e-03,  4.7349e-03,  4.9771e-03, -5.7101e-03,  1.0224e-02,\n",
       "                       -3.4176e-03,  5.2484e-03,  2.8712e-03, -7.6225e-03, -6.6854e-03,\n",
       "                       -5.0055e-03,  7.8670e-03,  2.7093e-03, -9.7316e-04, -3.8982e-03,\n",
       "                       -5.8937e-03, -1.8803e-03,  4.3387e-03,  5.2969e-03, -4.0216e-03,\n",
       "                        5.1929e-03,  3.6085e-03,  4.9256e-03, -2.5711e-03, -5.1120e-03,\n",
       "                        6.3266e-03,  1.3850e-03,  4.8710e-03, -5.7455e-03,  5.5146e-03,\n",
       "                        4.8414e-03, -4.6942e-03,  6.0969e-04, -5.3603e-03, -4.8442e-03,\n",
       "                       -7.2227e-03, -5.5758e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.0.attn.out_proj.weight',\n",
       "               tensor([[-0.0071, -0.0506,  0.0138,  ..., -0.0440, -0.0440, -0.1227],\n",
       "                       [-0.0856,  0.0830, -0.0365,  ..., -0.0239,  0.0680, -0.1059],\n",
       "                       [-0.0506, -0.0258, -0.0315,  ..., -0.0002, -0.0438,  0.0042],\n",
       "                       ...,\n",
       "                       [-0.1260,  0.0452,  0.1029,  ...,  0.0398, -0.0503,  0.0766],\n",
       "                       [ 0.0982,  0.0148,  0.0067,  ..., -0.1067, -0.0842, -0.0942],\n",
       "                       [-0.0247,  0.0006,  0.0667,  ...,  0.0662,  0.0247,  0.0399]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.attn.out_proj.bias',\n",
       "               tensor([ 0.0051,  0.0018, -0.0047, -0.0060,  0.0051, -0.0054, -0.0052, -0.0048,\n",
       "                        0.0003,  0.0042, -0.0055, -0.0019,  0.0049,  0.0045, -0.0027, -0.0066,\n",
       "                       -0.0051, -0.0049,  0.0060, -0.0039,  0.0070,  0.0011, -0.0052,  0.0039,\n",
       "                       -0.0021,  0.0051,  0.0048,  0.0042, -0.0044,  0.0059, -0.0054,  0.0051,\n",
       "                        0.0055,  0.0045,  0.0046,  0.0048, -0.0051,  0.0031, -0.0049,  0.0051,\n",
       "                        0.0055, -0.0054, -0.0049,  0.0046,  0.0045,  0.0058,  0.0044,  0.0049,\n",
       "                        0.0047,  0.0045,  0.0052,  0.0053, -0.0053, -0.0067,  0.0057, -0.0051,\n",
       "                       -0.0043, -0.0055, -0.0039, -0.0024, -0.0055, -0.0051, -0.0053, -0.0046],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.mlp.0.weight',\n",
       "               tensor([[-0.1085,  0.0576, -0.1057,  ..., -0.1108,  0.0116, -0.0284],\n",
       "                       [-0.0031, -0.0856,  0.0380,  ..., -0.0045, -0.0751,  0.0832],\n",
       "                       [ 0.0165,  0.0268, -0.0788,  ..., -0.0335,  0.0011,  0.1153],\n",
       "                       ...,\n",
       "                       [ 0.0183,  0.0486,  0.1170,  ...,  0.1087, -0.0736, -0.0460],\n",
       "                       [-0.0523, -0.0545, -0.0589,  ..., -0.0798, -0.1057,  0.0634],\n",
       "                       [ 0.0493, -0.0398,  0.0220,  ...,  0.0339, -0.1143,  0.0697]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.mlp.0.bias',\n",
       "               tensor([-0.1001,  0.0072,  0.0241, -0.0102, -0.0500,  0.0152,  0.0587,  0.0242,\n",
       "                        0.0909,  0.0720, -0.0317, -0.0734, -0.0130, -0.0737, -0.0732, -0.0571,\n",
       "                       -0.0997, -0.0681,  0.0053,  0.0672,  0.0110,  0.1207,  0.0315,  0.0264,\n",
       "                       -0.0700,  0.0628, -0.0128, -0.0862,  0.0886, -0.0916, -0.1038,  0.1023,\n",
       "                        0.0943,  0.1126, -0.0743, -0.0254, -0.0521,  0.1258, -0.0641,  0.0063,\n",
       "                        0.1126,  0.0772,  0.0928, -0.0136, -0.1088,  0.1188, -0.0997, -0.0920,\n",
       "                        0.0899,  0.0844,  0.0287,  0.0070,  0.0546, -0.0963, -0.0579, -0.0829,\n",
       "                       -0.0662, -0.0894, -0.0539,  0.0608,  0.0025, -0.0331,  0.0488,  0.0290,\n",
       "                        0.0791,  0.0565,  0.1052, -0.0941, -0.1079,  0.0298,  0.0628, -0.1137,\n",
       "                       -0.0537,  0.1054, -0.0880, -0.1156,  0.0941, -0.0272,  0.0765, -0.0559,\n",
       "                       -0.0586,  0.1158,  0.0975,  0.0108, -0.0769, -0.0093,  0.1302, -0.0408,\n",
       "                       -0.1036, -0.0319,  0.0132, -0.0624, -0.0003,  0.1092,  0.0627,  0.0802,\n",
       "                        0.0722,  0.0351,  0.1101, -0.0341, -0.0237,  0.0029, -0.0171,  0.0943,\n",
       "                        0.1284, -0.0301,  0.0822, -0.0095,  0.0522,  0.0647, -0.1119, -0.0859,\n",
       "                       -0.0426, -0.0811, -0.0107,  0.0312,  0.0029, -0.1196,  0.1000, -0.0333,\n",
       "                        0.0670,  0.0466, -0.0920,  0.1156, -0.0386, -0.1081, -0.0162, -0.1138,\n",
       "                        0.0515, -0.0118, -0.0239, -0.0150,  0.0373, -0.1201,  0.0238,  0.1153,\n",
       "                        0.0282,  0.0622,  0.0576,  0.0248,  0.0024, -0.0722,  0.0375, -0.0250,\n",
       "                       -0.0591, -0.0209, -0.0474, -0.1025,  0.1013, -0.0974, -0.0825,  0.0165,\n",
       "                        0.0829, -0.0105,  0.1072,  0.0723, -0.1061, -0.0472, -0.0191, -0.0139,\n",
       "                        0.1149,  0.0426, -0.0973,  0.0079,  0.1100,  0.1128,  0.0111,  0.0692,\n",
       "                        0.0631, -0.0379,  0.1035,  0.0913,  0.0771, -0.0400, -0.0491, -0.0488,\n",
       "                       -0.0299,  0.0309, -0.0036, -0.0962, -0.0601,  0.0490, -0.0319, -0.0418,\n",
       "                        0.0603, -0.0494,  0.1032, -0.0649,  0.0080, -0.0516, -0.0620, -0.0064,\n",
       "                       -0.0222,  0.0988, -0.0939,  0.0873,  0.0467, -0.0872, -0.0263, -0.1279,\n",
       "                        0.0753,  0.0609,  0.0581, -0.1008, -0.1214, -0.0436,  0.0292,  0.0292,\n",
       "                        0.1112,  0.1266,  0.0785, -0.0108, -0.0666,  0.0180,  0.0519,  0.0766,\n",
       "                       -0.0907, -0.0811, -0.0219, -0.0198, -0.0729,  0.0492, -0.0722,  0.0964,\n",
       "                        0.0344,  0.0420, -0.0268,  0.0565, -0.0040,  0.0589, -0.0439, -0.0974,\n",
       "                        0.0271,  0.1026, -0.0816,  0.0576,  0.0866, -0.1193, -0.0560, -0.0225,\n",
       "                       -0.0940, -0.0875, -0.1012,  0.1193, -0.0209, -0.0052,  0.1114,  0.0824,\n",
       "                       -0.0103,  0.0623,  0.1117,  0.0374, -0.0565, -0.0548,  0.0142, -0.0985],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.mlp.2.weight',\n",
       "               tensor([[ 0.0009, -0.0101, -0.0489,  ..., -0.0332,  0.0185, -0.0170],\n",
       "                       [-0.0603,  0.0511, -0.0355,  ..., -0.0438, -0.0273, -0.0540],\n",
       "                       [ 0.0072,  0.0194, -0.0224,  ...,  0.0202,  0.0484,  0.0547],\n",
       "                       ...,\n",
       "                       [ 0.0052,  0.0001, -0.0358,  ..., -0.0323,  0.0339,  0.0557],\n",
       "                       [-0.0312, -0.0269,  0.0001,  ...,  0.0474, -0.0496,  0.0422],\n",
       "                       [ 0.0207,  0.0003,  0.0400,  ...,  0.0272, -0.0123,  0.0047]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.mlp.2.bias',\n",
       "               tensor([ 0.0566,  0.0358, -0.0566,  0.0079,  0.0258, -0.0254,  0.0223, -0.0327,\n",
       "                       -0.0364, -0.0494, -0.0197,  0.0456,  0.0528, -0.0201, -0.0415,  0.0148,\n",
       "                       -0.0270,  0.0238,  0.0529, -0.0268, -0.0343,  0.0079,  0.0353,  0.0487,\n",
       "                       -0.0149, -0.0067, -0.0058,  0.0087, -0.0558,  0.0562,  0.0569, -0.0476,\n",
       "                       -0.0321,  0.0075, -0.0334, -0.0186,  0.0029,  0.0580, -0.0035,  0.0104,\n",
       "                       -0.0494, -0.0470, -0.0537,  0.0119,  0.0575, -0.0310, -0.0470,  0.0067,\n",
       "                       -0.0199,  0.0297, -0.0368, -0.0326, -0.0038, -0.0620,  0.0051, -0.0183,\n",
       "                        0.0181,  0.0089, -0.0055, -0.0510, -0.0418, -0.0677,  0.0045, -0.0671],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.ln_1.weight',\n",
       "               tensor([0.9999, 0.9946, 0.9990, 0.9964, 0.9943, 1.0015, 0.9949, 1.0059, 1.0065,\n",
       "                       1.0057, 0.9961, 0.9948, 1.0033, 1.0043, 0.9958, 1.0051, 0.9952, 1.0057,\n",
       "                       0.9972, 1.0064, 0.9984, 1.0074, 0.9951, 1.0051, 0.9951, 0.9955, 0.9951,\n",
       "                       1.0065, 1.0055, 1.0044, 0.9959, 0.9957, 1.0054, 0.9949, 0.9958, 1.0047,\n",
       "                       1.0040, 1.0060, 1.0052, 0.9979, 1.0038, 1.0048, 0.9962, 1.0021, 1.0078,\n",
       "                       0.9967, 1.0001, 1.0054, 0.9960, 1.0059, 1.0009, 1.0085, 0.9952, 1.0040,\n",
       "                       1.0040, 1.0053, 0.9954, 1.0031, 1.0053, 0.9965, 0.9976, 1.0053, 1.0087,\n",
       "                       0.9991], device='cuda:0')),\n",
       "              ('transformer.resblocks.0.ln_1.bias',\n",
       "               tensor([-0.0032, -0.0056, -0.0048,  0.0046,  0.0057, -0.0016,  0.0038,  0.0062,\n",
       "                       -0.0049, -0.0047,  0.0040, -0.0006,  0.0037, -0.0047, -0.0040, -0.0051,\n",
       "                       -0.0045, -0.0054, -0.0024,  0.0051, -0.0015,  0.0039,  0.0047, -0.0051,\n",
       "                       -0.0044, -0.0041,  0.0047, -0.0045,  0.0055,  0.0048, -0.0042, -0.0087,\n",
       "                       -0.0067,  0.0051,  0.0048, -0.0047, -0.0041, -0.0104,  0.0053,  0.0006,\n",
       "                        0.0061, -0.0049,  0.0065, -0.0053,  0.0049, -0.0072, -0.0088, -0.0054,\n",
       "                        0.0038,  0.0034,  0.0047, -0.0049, -0.0044,  0.0029, -0.0054,  0.0049,\n",
       "                       -0.0041,  0.0052, -0.0059, -0.0029,  0.0053,  0.0053,  0.0054, -0.0052],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.0.ln_2.weight',\n",
       "               tensor([1.0029, 1.0052, 0.9984, 0.9996, 0.9954, 0.9952, 1.0026, 1.0056, 1.0032,\n",
       "                       1.0070, 0.9993, 0.9988, 0.9957, 0.9991, 0.9975, 1.0038, 1.0064, 0.9960,\n",
       "                       0.9966, 0.9962, 1.0054, 1.0066, 1.0000, 1.0081, 1.0052, 1.0067, 0.9983,\n",
       "                       0.9962, 1.0056, 0.9964, 0.9976, 1.0069, 1.0054, 1.0060, 1.0057, 0.9958,\n",
       "                       0.9971, 0.9949, 0.9993, 1.0055, 1.0095, 0.9962, 1.0138, 0.9947, 0.9991,\n",
       "                       1.0087, 1.0099, 1.0055, 0.9954, 0.9958, 0.9974, 1.0119, 1.0045, 1.0022,\n",
       "                       0.9953, 0.9948, 0.9972, 1.0010, 1.0052, 0.9952, 1.0066, 0.9996, 1.0056,\n",
       "                       0.9979], device='cuda:0')),\n",
       "              ('transformer.resblocks.0.ln_2.bias',\n",
       "               tensor([ 0.0071,  0.0050, -0.0038, -0.0031,  0.0052, -0.0053,  0.0016,  0.0022,\n",
       "                       -0.0030, -0.0045,  0.0030,  0.0059,  0.0076,  0.0058, -0.0013,  0.0052,\n",
       "                       -0.0058,  0.0014,  0.0068,  0.0046,  0.0055, -0.0057, -0.0006, -0.0027,\n",
       "                        0.0056,  0.0065,  0.0024, -0.0043,  0.0058, -0.0037, -0.0070,  0.0081,\n",
       "                       -0.0057, -0.0052,  0.0047,  0.0045,  0.0044,  0.0042, -0.0056,  0.0060,\n",
       "                       -0.0076, -0.0065, -0.0049,  0.0011, -0.0084,  0.0061, -0.0065, -0.0046,\n",
       "                       -0.0070,  0.0050,  0.0070,  0.0107,  0.0042, -0.0076, -0.0043,  0.0080,\n",
       "                        0.0054,  0.0040,  0.0055, -0.0045,  0.0007, -0.0013,  0.0036, -0.0053],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.attn.in_proj_weight',\n",
       "               tensor([[-0.1230,  0.1366,  0.1462,  ...,  0.1340,  0.1051,  0.1292],\n",
       "                       [-0.1358,  0.0154, -0.1168,  ..., -0.1172, -0.0057, -0.1363],\n",
       "                       [-0.1125, -0.0708,  0.0555,  ..., -0.1020,  0.0097, -0.0449],\n",
       "                       ...,\n",
       "                       [-0.0907, -0.1417, -0.0165,  ..., -0.1129, -0.0227, -0.0006],\n",
       "                       [ 0.1098,  0.0579,  0.1542,  ..., -0.1242,  0.0283,  0.0922],\n",
       "                       [ 0.0493, -0.0377, -0.1157,  ...,  0.0923, -0.0414, -0.0301]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.attn.in_proj_bias',\n",
       "               tensor([ 5.0839e-03,  3.2845e-03,  6.1343e-03, -4.6816e-03,  5.2950e-03,\n",
       "                       -4.8698e-03, -5.0744e-03,  3.9583e-03, -4.6450e-03,  6.5055e-03,\n",
       "                       -5.2210e-03,  5.0659e-03,  4.9696e-03, -6.4140e-04, -4.9941e-03,\n",
       "                        4.7429e-03, -3.6929e-03,  4.0859e-03, -1.4940e-04, -4.7814e-03,\n",
       "                        6.1380e-03, -5.6448e-03, -4.9285e-03, -4.0623e-03,  3.5891e-03,\n",
       "                       -1.7424e-03, -3.2022e-04,  3.0777e-03, -5.4130e-03, -4.7957e-03,\n",
       "                        4.5702e-03,  4.9508e-03, -6.5523e-03,  4.9898e-03, -2.2525e-03,\n",
       "                       -4.2044e-03, -5.1295e-03,  4.6406e-03, -5.4563e-03,  5.5594e-03,\n",
       "                       -2.8273e-03,  4.3625e-03,  5.3228e-03, -6.2100e-03, -4.5751e-03,\n",
       "                        4.3894e-03,  4.4112e-03, -6.0353e-03,  5.8122e-03,  9.9010e-04,\n",
       "                        3.7955e-03, -5.7539e-03, -5.6951e-03,  4.5846e-03,  4.6981e-03,\n",
       "                        2.6666e-03, -5.1302e-03, -5.3329e-03,  5.4270e-03,  5.2662e-03,\n",
       "                       -4.8971e-03,  4.5241e-03, -5.0877e-03,  5.2212e-03,  3.4545e-06,\n",
       "                        6.7632e-06,  2.3621e-06, -1.9988e-06, -6.1563e-06, -1.0399e-06,\n",
       "                        1.4040e-06,  7.4057e-07, -1.8592e-05,  1.6914e-05, -5.2809e-06,\n",
       "                        1.6782e-05, -3.1658e-06, -1.5527e-06,  4.8488e-06,  2.7862e-07,\n",
       "                        1.5624e-06, -6.9634e-06,  1.5607e-06, -7.2581e-06,  1.2013e-05,\n",
       "                       -8.7686e-06, -2.3882e-05, -1.3951e-05,  9.4195e-07,  4.5918e-06,\n",
       "                       -1.0747e-06,  5.5572e-06,  3.0671e-07,  1.1204e-05, -3.1906e-06,\n",
       "                        3.9339e-05, -2.3611e-05,  9.1665e-06,  2.6955e-06,  1.9974e-07,\n",
       "                       -7.4810e-06, -1.1701e-05, -1.3441e-05,  4.9416e-07, -8.2027e-06,\n",
       "                       -2.5891e-05, -5.2651e-06,  9.1160e-06, -9.0255e-07, -1.1475e-06,\n",
       "                       -2.7441e-05, -1.1777e-05, -6.9979e-07,  7.8277e-06,  4.1193e-05,\n",
       "                        5.3062e-06, -1.1661e-05,  4.5995e-06,  1.2902e-05,  2.2571e-05,\n",
       "                        9.5438e-07,  3.0843e-06, -9.2167e-06,  9.3462e-06, -5.1172e-05,\n",
       "                        1.7268e-06, -1.1883e-05,  6.3797e-07, -4.4913e-03, -5.1765e-03,\n",
       "                        4.7771e-03, -4.0387e-03, -3.9578e-03, -4.9443e-03,  5.0471e-03,\n",
       "                        6.2365e-03, -5.0678e-03,  6.0488e-03,  5.0364e-03,  5.1130e-03,\n",
       "                       -2.0803e-03, -5.0930e-03, -5.0442e-03, -4.4977e-03, -4.2077e-03,\n",
       "                       -5.0409e-03, -5.3879e-03,  5.0784e-03,  6.1753e-03, -3.9606e-03,\n",
       "                        5.0292e-03, -5.2414e-03,  4.9360e-03,  4.7367e-03, -4.9049e-03,\n",
       "                       -4.7679e-03,  5.6268e-03, -6.3966e-03,  4.5045e-03,  5.1062e-03,\n",
       "                       -4.8174e-03,  4.0292e-03, -5.6881e-03,  5.1155e-03,  6.0355e-03,\n",
       "                       -3.1367e-03, -9.2033e-03, -4.9324e-03, -4.7633e-03, -3.0131e-03,\n",
       "                       -5.0225e-03, -4.7570e-03,  5.5169e-03, -4.8514e-03, -5.0546e-03,\n",
       "                        3.3229e-03, -5.9509e-03, -1.5057e-03,  6.4610e-03,  4.5488e-03,\n",
       "                       -4.8314e-03,  1.1876e-03,  4.4165e-03, -5.2784e-03, -4.9837e-03,\n",
       "                       -3.3357e-03,  4.5807e-03, -5.1919e-03, -5.6874e-03, -5.2725e-03,\n",
       "                       -5.1621e-03, -1.5740e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.1.attn.out_proj.weight',\n",
       "               tensor([[ 0.1057,  0.0759,  0.0251,  ...,  0.0866,  0.1163,  0.0577],\n",
       "                       [-0.0417, -0.1218,  0.0341,  ..., -0.0764, -0.0160, -0.0386],\n",
       "                       [-0.0172, -0.0401, -0.0702,  ...,  0.0058, -0.0237,  0.0863],\n",
       "                       ...,\n",
       "                       [-0.0122, -0.0078, -0.1059,  ...,  0.0384,  0.1080,  0.1097],\n",
       "                       [ 0.1029, -0.0287, -0.1083,  ...,  0.0083,  0.0704,  0.0692],\n",
       "                       [ 0.0529,  0.1141,  0.0570,  ..., -0.0696, -0.0681, -0.1044]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.attn.out_proj.bias',\n",
       "               tensor([ 0.0050,  0.0089, -0.0047, -0.0064, -0.0049, -0.0057, -0.0052, -0.0049,\n",
       "                        0.0046,  0.0042, -0.0051, -0.0040,  0.0049,  0.0015, -0.0031, -0.0060,\n",
       "                       -0.0045, -0.0047,  0.0057, -0.0044,  0.0004,  0.0034, -0.0065, -0.0015,\n",
       "                       -0.0029,  0.0053,  0.0048,  0.0044, -0.0045,  0.0055, -0.0052,  0.0049,\n",
       "                       -0.0043,  0.0046,  0.0048,  0.0047, -0.0050,  0.0039, -0.0049,  0.0051,\n",
       "                        0.0067, -0.0051, -0.0048,  0.0035,  0.0048, -0.0049,  0.0046,  0.0050,\n",
       "                        0.0049,  0.0034,  0.0051,  0.0049, -0.0037, -0.0061,  0.0053, -0.0080,\n",
       "                       -0.0044,  0.0048, -0.0039,  0.0066, -0.0049, -0.0083, -0.0052, -0.0046],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.mlp.0.weight',\n",
       "               tensor([[ 7.4923e-02,  6.6421e-02,  5.6339e-02,  ..., -4.6705e-02,\n",
       "                        -3.5263e-05,  3.5975e-02],\n",
       "                       [ 1.1010e-01,  1.1389e-02,  7.2266e-02,  ..., -8.9753e-02,\n",
       "                         3.3710e-02, -1.8101e-02],\n",
       "                       [-5.8212e-02, -8.7047e-02,  9.6157e-02,  ..., -1.4321e-02,\n",
       "                         6.3277e-03,  9.0506e-02],\n",
       "                       ...,\n",
       "                       [-2.6289e-02, -7.5230e-02,  3.4894e-02,  ...,  5.2516e-02,\n",
       "                         1.0993e-01, -2.3287e-02],\n",
       "                       [-7.8708e-02, -4.4173e-02, -9.7815e-02,  ..., -9.5522e-02,\n",
       "                        -7.9971e-02, -1.1872e-01],\n",
       "                       [-5.9146e-02,  5.2421e-02, -6.6640e-02,  ...,  2.3157e-02,\n",
       "                         1.0888e-02,  6.6744e-02]], device='cuda:0')),\n",
       "              ('transformer.resblocks.1.mlp.0.bias',\n",
       "               tensor([ 0.0749, -0.0525,  0.0212,  0.1123, -0.0310,  0.0558,  0.0571, -0.0104,\n",
       "                        0.0601, -0.0559,  0.0927,  0.0190,  0.0539,  0.0975, -0.0058, -0.0554,\n",
       "                       -0.1074, -0.0001,  0.1207,  0.1124,  0.0885,  0.1168, -0.0770, -0.1216,\n",
       "                        0.0508,  0.1222,  0.0004, -0.0761, -0.0580,  0.0286,  0.0037,  0.0573,\n",
       "                        0.0740, -0.0986,  0.0512,  0.0658,  0.0585, -0.1088, -0.0168, -0.0159,\n",
       "                       -0.0723,  0.0117, -0.1060, -0.0468,  0.1083,  0.0680,  0.0218, -0.0458,\n",
       "                       -0.0930,  0.0129,  0.0368, -0.0119,  0.1063,  0.0890, -0.0883, -0.0468,\n",
       "                        0.0432,  0.1247, -0.0592, -0.1158, -0.0119, -0.0407, -0.0478,  0.0965,\n",
       "                        0.0926,  0.0294, -0.0753, -0.0734,  0.0568, -0.0677,  0.1242,  0.0955,\n",
       "                       -0.0088,  0.0271,  0.0849,  0.1078,  0.0945, -0.0250,  0.0515, -0.0202,\n",
       "                       -0.0900,  0.0642, -0.0366, -0.0291,  0.1123, -0.1184,  0.0541,  0.1035,\n",
       "                       -0.0401,  0.1124, -0.0581, -0.0769,  0.0831,  0.0893,  0.0244, -0.0667,\n",
       "                       -0.0952,  0.0420, -0.0417,  0.1165,  0.0552,  0.0890, -0.1118,  0.0192,\n",
       "                       -0.0460, -0.0656,  0.1144,  0.0203, -0.0118,  0.1253,  0.0664, -0.0347,\n",
       "                        0.1159, -0.0902,  0.0351,  0.0031,  0.0046,  0.0588, -0.0549, -0.0409,\n",
       "                       -0.0038,  0.0233,  0.0562, -0.0336,  0.0864, -0.0350,  0.0023, -0.0362,\n",
       "                       -0.0187,  0.0781,  0.0996, -0.0726, -0.0321, -0.0943, -0.0335,  0.1300,\n",
       "                        0.0226,  0.0077, -0.0969,  0.0815,  0.0404,  0.0054,  0.0059,  0.0975,\n",
       "                       -0.0132,  0.0366,  0.0707, -0.1182,  0.0561, -0.1181,  0.1002, -0.0727,\n",
       "                        0.0899, -0.0645,  0.0232, -0.0753,  0.0531,  0.1158,  0.0493, -0.0871,\n",
       "                       -0.1202,  0.1113,  0.1101,  0.0156,  0.1106, -0.1204, -0.0806, -0.0526,\n",
       "                       -0.0065,  0.0130, -0.0029,  0.0749,  0.0920, -0.1107,  0.0630, -0.0102,\n",
       "                        0.1245, -0.0321,  0.1202,  0.0526, -0.0036, -0.0540, -0.0701, -0.0750,\n",
       "                       -0.0502,  0.0972,  0.0356,  0.0391,  0.0591,  0.0926, -0.1221,  0.0094,\n",
       "                        0.0765,  0.0509,  0.0025, -0.0913, -0.0842, -0.0474,  0.0758,  0.0729,\n",
       "                       -0.0434, -0.0956, -0.1068, -0.0710,  0.0890, -0.0440, -0.0306,  0.0967,\n",
       "                        0.0931,  0.0120,  0.0776,  0.0626, -0.0716,  0.0193, -0.0557,  0.0749,\n",
       "                        0.1227,  0.0673, -0.0964,  0.0563, -0.0555,  0.0570,  0.1115, -0.0770,\n",
       "                       -0.0785, -0.0753,  0.0748,  0.0152, -0.0595, -0.1036,  0.1219, -0.0137,\n",
       "                       -0.0273,  0.0468,  0.1223,  0.1175, -0.0847, -0.0285,  0.0661, -0.0352,\n",
       "                        0.0029,  0.0648,  0.0950,  0.0562, -0.0704,  0.0370,  0.0120, -0.0087,\n",
       "                        0.0192, -0.0760, -0.0506, -0.0778, -0.0202, -0.1199,  0.0980,  0.1151],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.mlp.2.weight',\n",
       "               tensor([[-0.0390, -0.0468,  0.0170,  ..., -0.0419,  0.0526,  0.0030],\n",
       "                       [-0.0458,  0.0626, -0.0002,  ...,  0.0111, -0.0017, -0.0322],\n",
       "                       [ 0.0105, -0.0069,  0.0275,  ..., -0.0256, -0.0623, -0.0205],\n",
       "                       ...,\n",
       "                       [ 0.0474,  0.0013,  0.0337,  ...,  0.0298, -0.0319,  0.0115],\n",
       "                       [ 0.0324,  0.0158,  0.0658,  ...,  0.0102,  0.0256, -0.0646],\n",
       "                       [-0.0559,  0.0671,  0.0582,  ...,  0.0541, -0.0392,  0.0451]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.mlp.2.bias',\n",
       "               tensor([ 0.0377,  0.0028,  0.0407,  0.0286,  0.0551, -0.0380, -0.0406, -0.0543,\n",
       "                        0.0284,  0.0644, -0.0217,  0.0014,  0.0380,  0.0597,  0.0114, -0.0598,\n",
       "                       -0.0224,  0.0468, -0.0400, -0.0420, -0.0147, -0.0315, -0.0099, -0.0117,\n",
       "                       -0.0481,  0.0525,  0.0450,  0.0393,  0.0145,  0.0584, -0.0253,  0.0673,\n",
       "                       -0.0430,  0.0118,  0.0254,  0.0088,  0.0036,  0.0489,  0.0248,  0.0353,\n",
       "                       -0.0005, -0.0389,  0.0532, -0.0044, -0.0071,  0.0076,  0.0590,  0.0455,\n",
       "                       -0.0054, -0.0194,  0.0005, -0.0138, -0.0125,  0.0103,  0.0107, -0.0490,\n",
       "                       -0.0459, -0.0173,  0.0275,  0.0443, -0.0038, -0.0380,  0.0159, -0.0472],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.ln_1.weight',\n",
       "               tensor([1.0049, 0.9953, 0.9957, 1.0053, 0.9970, 1.0062, 1.0087, 1.0034, 1.0039,\n",
       "                       0.9961, 0.9971, 1.0065, 1.0039, 1.0070, 0.9965, 1.0060, 1.0066, 1.0081,\n",
       "                       1.0072, 1.0055, 1.0111, 0.9962, 1.0039, 0.9983, 0.9975, 1.0043, 1.0002,\n",
       "                       1.0059, 0.9954, 1.0080, 1.0028, 1.0082, 1.0066, 0.9966, 0.9956, 0.9974,\n",
       "                       0.9980, 0.9970, 0.9961, 1.0045, 0.9946, 0.9962, 1.0074, 1.0049, 0.9955,\n",
       "                       1.0016, 0.9963, 0.9961, 0.9950, 1.0001, 0.9995, 0.9962, 1.0062, 1.0055,\n",
       "                       0.9994, 1.0074, 0.9954, 1.0032, 0.9953, 0.9966, 0.9971, 0.9958, 1.0047,\n",
       "                       1.0049], device='cuda:0')),\n",
       "              ('transformer.resblocks.1.ln_1.bias',\n",
       "               tensor([-0.0031, -0.0050, -0.0049, -0.0049,  0.0047, -0.0051, -0.0052,  0.0056,\n",
       "                       -0.0060,  0.0054,  0.0051, -0.0009,  0.0049,  0.0047,  0.0061, -0.0066,\n",
       "                       -0.0056, -0.0058,  0.0036, -0.0033,  0.0077,  0.0063, -0.0044,  0.0047,\n",
       "                       -0.0044,  0.0047,  0.0048,  0.0045, -0.0048,  0.0055,  0.0047,  0.0058,\n",
       "                        0.0050,  0.0038,  0.0043,  0.0054,  0.0029, -0.0060, -0.0047, -0.0056,\n",
       "                        0.0050,  0.0041, -0.0055,  0.0063, -0.0048,  0.0050,  0.0049,  0.0042,\n",
       "                       -0.0047, -0.0048,  0.0050, -0.0041, -0.0054,  0.0052, -0.0052, -0.0051,\n",
       "                       -0.0045, -0.0051, -0.0044, -0.0038, -0.0020, -0.0042, -0.0039, -0.0047],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.1.ln_2.weight',\n",
       "               tensor([1.0049, 1.0059, 1.0079, 0.9976, 1.0059, 1.0078, 0.9963, 1.0060, 1.0083,\n",
       "                       1.0057, 1.0005, 1.0063, 1.0049, 0.9951, 0.9967, 0.9924, 0.9945, 0.9984,\n",
       "                       1.0124, 0.9971, 1.0084, 1.0055, 1.0068, 1.0028, 1.0063, 1.0086, 0.9955,\n",
       "                       0.9941, 0.9949, 1.0011, 0.9975, 1.0029, 0.9981, 0.9953, 1.0061, 1.0056,\n",
       "                       1.0055, 0.9967, 0.9988, 0.9965, 0.9961, 1.0095, 1.0063, 1.0065, 1.0050,\n",
       "                       1.0054, 1.0046, 1.0071, 0.9983, 1.0055, 1.0054, 1.0051, 1.0053, 0.9977,\n",
       "                       1.0058, 1.0065, 1.0066, 1.0068, 1.0058, 0.9964, 0.9969, 1.0057, 1.0071,\n",
       "                       1.0067], device='cuda:0')),\n",
       "              ('transformer.resblocks.1.ln_2.bias',\n",
       "               tensor([ 0.0070,  0.0055, -0.0056,  0.0019, -0.0057, -0.0076, -0.0047, -0.0056,\n",
       "                       -0.0043,  0.0056,  0.0016,  0.0060,  0.0053, -0.0047, -0.0047,  0.0057,\n",
       "                        0.0056,  0.0059,  0.0059, -0.0041,  0.0071, -0.0071,  0.0057,  0.0102,\n",
       "                        0.0058,  0.0107, -0.0044, -0.0029,  0.0053, -0.0026, -0.0058,  0.0076,\n",
       "                       -0.0056,  0.0053,  0.0049, -0.0080,  0.0030,  0.0041,  0.0030,  0.0074,\n",
       "                        0.0048, -0.0058, -0.0062, -0.0064, -0.0083,  0.0024, -0.0049, -0.0021,\n",
       "                        0.0052,  0.0046, -0.0056,  0.0062, -0.0054,  0.0050,  0.0062, -0.0059,\n",
       "                       -0.0055,  0.0054,  0.0063, -0.0035,  0.0046,  0.0063, -0.0058, -0.0056],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.attn.in_proj_weight',\n",
       "               tensor([[ 0.1170, -0.0336, -0.0591,  ..., -0.0648, -0.0314,  0.0445],\n",
       "                       [-0.0537,  0.1131, -0.0670,  ..., -0.1087, -0.0809, -0.0767],\n",
       "                       [-0.0489,  0.1374, -0.0575,  ..., -0.1504,  0.1265,  0.1193],\n",
       "                       ...,\n",
       "                       [ 0.0167, -0.0075, -0.1446,  ..., -0.1470,  0.0612,  0.0864],\n",
       "                       [-0.0335,  0.0348, -0.0993,  ..., -0.1295,  0.0446, -0.0720],\n",
       "                       [-0.1178,  0.0969, -0.0864,  ...,  0.0934, -0.1515,  0.0113]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.attn.in_proj_bias',\n",
       "               tensor([-4.9150e-03, -4.7453e-03,  3.1674e-03,  5.8896e-03,  2.3895e-03,\n",
       "                        3.3631e-03, -5.8959e-03,  5.1631e-03,  4.5721e-03,  2.4464e-03,\n",
       "                       -4.8180e-03, -2.5522e-03,  4.9621e-03, -4.4390e-03,  5.7166e-03,\n",
       "                        5.5587e-03, -4.0424e-03, -6.2628e-03,  2.1148e-04,  5.5464e-03,\n",
       "                       -9.2148e-03, -4.3727e-03,  4.9116e-03,  5.1020e-03, -4.7352e-03,\n",
       "                        5.8438e-03,  4.2682e-03, -4.0884e-03,  3.7672e-03,  4.6892e-03,\n",
       "                       -4.3278e-03, -2.1318e-03,  4.9451e-03, -4.0208e-03,  4.7531e-03,\n",
       "                       -5.9015e-03,  4.8807e-03, -4.9763e-03,  6.7730e-03, -7.2732e-03,\n",
       "                       -7.2236e-03, -5.5046e-03, -4.5195e-03, -4.6476e-03,  4.4670e-03,\n",
       "                       -6.3210e-03,  5.6559e-03, -4.8968e-03,  5.0895e-03,  4.9145e-03,\n",
       "                       -4.1885e-03,  5.1320e-03,  5.0976e-03, -3.4158e-03, -7.3613e-03,\n",
       "                       -4.6687e-03, -5.7234e-04, -4.4314e-03, -4.9420e-03, -3.1426e-03,\n",
       "                       -3.9021e-03, -4.9606e-03, -4.4075e-03, -4.6402e-03,  1.0666e-05,\n",
       "                        5.9019e-06, -1.4116e-05, -5.3316e-06, -1.8321e-05, -1.1485e-05,\n",
       "                       -1.7622e-05, -1.7265e-06, -9.5993e-06,  3.5609e-06, -3.3188e-06,\n",
       "                        2.0457e-05, -7.0410e-06,  6.9636e-06, -6.2767e-06,  2.6626e-06,\n",
       "                       -7.7154e-06,  1.7774e-05,  6.4815e-06, -8.3863e-06,  1.3125e-05,\n",
       "                        2.5108e-05, -7.5000e-07, -4.0773e-05,  1.3011e-05,  9.2147e-06,\n",
       "                        1.1161e-05, -2.4106e-05,  7.7160e-06,  6.3504e-06, -5.0280e-07,\n",
       "                        1.4731e-05,  8.9798e-06,  2.8568e-05, -4.1097e-06,  2.7793e-05,\n",
       "                        3.0890e-06, -1.1572e-06, -1.4099e-05, -1.0856e-05,  3.9004e-06,\n",
       "                       -1.6946e-05, -1.7966e-05, -1.8656e-05,  1.5760e-05,  2.0442e-06,\n",
       "                       -7.0331e-09, -5.6034e-06, -4.5968e-06, -3.8991e-06,  3.5510e-06,\n",
       "                       -1.9693e-06,  1.0429e-06, -6.6925e-06,  7.1771e-07, -1.0152e-05,\n",
       "                       -2.9468e-06,  6.0554e-07,  1.8871e-06, -5.8118e-06,  2.1656e-06,\n",
       "                       -9.1114e-08, -4.8105e-06,  1.0439e-05,  4.5608e-04,  4.2253e-03,\n",
       "                        2.0064e-03,  2.5950e-03,  5.3717e-03,  4.5766e-03,  5.2671e-03,\n",
       "                        4.5301e-03,  5.1651e-03, -4.7060e-03,  4.5494e-03, -5.7972e-03,\n",
       "                        2.9054e-03, -4.3799e-03,  5.0423e-03,  6.7185e-03, -4.1553e-03,\n",
       "                        4.5014e-03,  4.9297e-03,  4.7585e-03,  4.8179e-03, -3.6482e-03,\n",
       "                       -2.9651e-03, -4.8347e-03,  5.2711e-03,  4.8340e-03,  5.1815e-03,\n",
       "                       -5.0859e-03,  4.7563e-03,  4.7544e-03,  2.7193e-03,  4.3044e-03,\n",
       "                        4.3500e-03,  2.1553e-03,  4.1718e-03,  4.9833e-03,  5.1499e-03,\n",
       "                       -4.7581e-03,  4.6661e-03,  8.8454e-03, -5.0243e-03, -1.1666e-02,\n",
       "                       -4.9624e-03, -5.8222e-03,  4.7973e-03,  5.4777e-03,  4.9515e-03,\n",
       "                        3.6411e-03,  5.2705e-03, -5.1845e-03, -4.5930e-03,  3.9821e-03,\n",
       "                        2.9642e-03,  4.8935e-03,  4.9238e-03, -4.9858e-03,  4.7322e-03,\n",
       "                        4.5665e-03,  5.6714e-03, -4.4718e-03,  4.8832e-03, -4.8628e-03,\n",
       "                        4.5985e-03, -4.4819e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.2.attn.out_proj.weight',\n",
       "               tensor([[-0.0106, -0.0829, -0.0470,  ...,  0.0874, -0.1027, -0.1190],\n",
       "                       [-0.0032, -0.1049, -0.1170,  ...,  0.0401, -0.0829,  0.0338],\n",
       "                       [-0.0841, -0.1054, -0.1150,  ..., -0.1000,  0.0660, -0.0273],\n",
       "                       ...,\n",
       "                       [ 0.0304,  0.0190,  0.0048,  ..., -0.0609,  0.0290, -0.1205],\n",
       "                       [ 0.0296,  0.0765, -0.1031,  ..., -0.0272, -0.0843, -0.0117],\n",
       "                       [-0.1158, -0.0148, -0.1239,  ..., -0.0917,  0.1239, -0.0318]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.attn.out_proj.bias',\n",
       "               tensor([ 0.0050, -0.0034, -0.0042, -0.0051, -0.0058, -0.0054, -0.0053, -0.0050,\n",
       "                        0.0049,  0.0041, -0.0051, -0.0059,  0.0047,  0.0041, -0.0033, -0.0056,\n",
       "                       -0.0051, -0.0046,  0.0055, -0.0045, -0.0019,  0.0020,  0.0042,  0.0039,\n",
       "                        0.0068,  0.0051,  0.0048,  0.0047, -0.0046,  0.0051, -0.0049,  0.0047,\n",
       "                        0.0059,  0.0042,  0.0053,  0.0048, -0.0051,  0.0034, -0.0051,  0.0051,\n",
       "                        0.0076, -0.0051, -0.0047,  0.0009,  0.0003, -0.0049,  0.0050,  0.0049,\n",
       "                        0.0050,  0.0048,  0.0051,  0.0049,  0.0079, -0.0052,  0.0054,  0.0049,\n",
       "                       -0.0030,  0.0052, -0.0046,  0.0055, -0.0047,  0.0002, -0.0052, -0.0044],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.mlp.0.weight',\n",
       "               tensor([[ 0.0977,  0.0997,  0.0679,  ...,  0.0996,  0.0434, -0.0489],\n",
       "                       [-0.0608, -0.0791,  0.0428,  ..., -0.0344,  0.0211,  0.0294],\n",
       "                       [-0.0811, -0.0052, -0.1159,  ...,  0.0489, -0.1044, -0.0993],\n",
       "                       ...,\n",
       "                       [ 0.0926,  0.0205,  0.0276,  ...,  0.0486,  0.0241, -0.1218],\n",
       "                       [ 0.0239, -0.0621, -0.0072,  ..., -0.0096,  0.0346, -0.1000],\n",
       "                       [ 0.0616, -0.0436,  0.1082,  ...,  0.0675,  0.0540, -0.0690]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.mlp.0.bias',\n",
       "               tensor([ 0.0187, -0.1172,  0.0573, -0.0482, -0.0151,  0.0528,  0.0783, -0.0461,\n",
       "                        0.0684,  0.0572,  0.0009,  0.0437,  0.1033,  0.0756,  0.0606, -0.0829,\n",
       "                       -0.0526, -0.0283, -0.0177, -0.0464, -0.0113,  0.0716, -0.0280, -0.1204,\n",
       "                        0.0742, -0.1285,  0.1098, -0.0505,  0.0151,  0.0967, -0.0064,  0.1012,\n",
       "                        0.0552, -0.1211, -0.0793, -0.0006,  0.1053, -0.0107,  0.0569, -0.0532,\n",
       "                        0.1273, -0.0193,  0.0582,  0.0830, -0.0290,  0.1019,  0.0418,  0.0037,\n",
       "                       -0.1276, -0.0862,  0.1043,  0.0512,  0.0163, -0.0003, -0.0317,  0.1053,\n",
       "                        0.0575,  0.0217,  0.1141,  0.1159,  0.0480, -0.0350,  0.1188,  0.1250,\n",
       "                        0.0277, -0.0498, -0.0354,  0.0609,  0.0235,  0.0704, -0.0342, -0.0635,\n",
       "                        0.0270,  0.0927, -0.0961,  0.0436,  0.0695, -0.0486,  0.0389, -0.0729,\n",
       "                        0.0631,  0.1122, -0.0493, -0.1094, -0.1250,  0.0474,  0.1242, -0.0949,\n",
       "                       -0.0353, -0.0427,  0.0771,  0.0611, -0.0280, -0.0462, -0.0794, -0.0427,\n",
       "                       -0.0416,  0.0193,  0.0902, -0.0642, -0.1033, -0.0352,  0.0746, -0.0098,\n",
       "                        0.0972, -0.0582,  0.0633,  0.1125,  0.0538, -0.0299,  0.0454, -0.0859,\n",
       "                        0.0979, -0.0429,  0.0994, -0.0504, -0.0355,  0.0564, -0.1121, -0.0551,\n",
       "                       -0.0992, -0.0237,  0.0418,  0.0926,  0.1164,  0.1230,  0.0432, -0.1058,\n",
       "                        0.1123,  0.1174,  0.0396,  0.0392, -0.0162,  0.0453,  0.0034, -0.1078,\n",
       "                       -0.0604, -0.0150,  0.1154,  0.0268,  0.0641, -0.0336,  0.1197,  0.0433,\n",
       "                        0.0401,  0.0087, -0.0561, -0.0724,  0.0188, -0.0660,  0.1267,  0.0018,\n",
       "                       -0.0821,  0.0500, -0.0351, -0.0439, -0.0352,  0.0189, -0.0479,  0.0312,\n",
       "                       -0.0112,  0.1171, -0.0029, -0.0416,  0.0131,  0.0450,  0.0111, -0.0408,\n",
       "                       -0.1262,  0.0042, -0.0220,  0.0528,  0.0844, -0.0451,  0.1099,  0.0732,\n",
       "                        0.1139,  0.0203, -0.0730,  0.1107,  0.0082, -0.0161,  0.0571,  0.0732,\n",
       "                       -0.0656,  0.0457, -0.0246,  0.1279, -0.1016, -0.0138, -0.0235, -0.0496,\n",
       "                       -0.0330, -0.0741, -0.0487, -0.0213, -0.0242,  0.0791, -0.0950,  0.0837,\n",
       "                        0.0398, -0.1199, -0.0973,  0.1291,  0.1155, -0.0800, -0.0444,  0.0122,\n",
       "                       -0.1010, -0.1208,  0.0363, -0.1063,  0.0957, -0.0107, -0.0780,  0.1084,\n",
       "                        0.1157,  0.0143, -0.0257,  0.1031,  0.0259,  0.1155,  0.1207,  0.0839,\n",
       "                        0.0140,  0.0969, -0.0760, -0.0758,  0.0523, -0.0270, -0.0265, -0.0053,\n",
       "                       -0.0396,  0.0235, -0.0659,  0.0645,  0.0447, -0.0079, -0.0962, -0.0268,\n",
       "                       -0.0898, -0.0350, -0.1282, -0.0663, -0.0855,  0.0648, -0.0007,  0.1014,\n",
       "                        0.0430, -0.0220,  0.0374, -0.1255,  0.0256,  0.0316,  0.0526,  0.1134],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.mlp.2.weight',\n",
       "               tensor([[-0.0196, -0.0152, -0.0317,  ..., -0.0025, -0.0603, -0.0246],\n",
       "                       [-0.0040,  0.0437, -0.0124,  ...,  0.0482,  0.0157, -0.0081],\n",
       "                       [ 0.0202,  0.0198,  0.0075,  ..., -0.0509,  0.0379,  0.0036],\n",
       "                       ...,\n",
       "                       [ 0.0058,  0.0111,  0.0196,  ..., -0.0236,  0.0454,  0.0298],\n",
       "                       [ 0.0312,  0.0282,  0.0323,  ..., -0.0205,  0.0595,  0.0555],\n",
       "                       [-0.0363, -0.0317,  0.0621,  ..., -0.0105,  0.0317,  0.0649]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.mlp.2.bias',\n",
       "               tensor([-8.9715e-03, -2.6689e-02, -6.1139e-03,  3.9624e-02,  7.2771e-03,\n",
       "                        4.0974e-02, -6.2068e-02, -1.0753e-02,  3.3100e-02,  3.0589e-03,\n",
       "                        5.6227e-02, -4.8761e-02,  4.1426e-02,  5.6318e-02, -2.6046e-02,\n",
       "                       -5.5612e-02,  5.1298e-05, -9.1998e-03,  5.4430e-02,  2.0061e-02,\n",
       "                        8.2565e-04,  7.5965e-03,  5.7549e-02, -3.8055e-02, -3.3959e-02,\n",
       "                       -1.2448e-02, -1.7925e-02, -7.5760e-03,  5.2551e-02,  6.4232e-02,\n",
       "                       -1.4555e-02,  4.4883e-02,  1.9672e-02,  4.6522e-02,  3.2096e-02,\n",
       "                        2.4485e-02, -2.9484e-02,  1.9974e-02, -5.1073e-02,  2.8042e-02,\n",
       "                        6.9576e-02, -2.1104e-02,  1.1510e-02,  4.1961e-02, -9.5585e-03,\n",
       "                       -3.3348e-02,  3.8596e-02,  3.1189e-02,  3.9065e-02, -1.3801e-02,\n",
       "                        5.9854e-02,  4.0552e-02, -4.3813e-02, -1.3669e-02,  8.0918e-03,\n",
       "                        1.6920e-02,  2.0576e-02, -1.7861e-02,  3.5538e-02, -4.4165e-02,\n",
       "                        8.3320e-03, -4.6363e-02,  5.5699e-02,  5.5049e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.2.ln_1.weight',\n",
       "               tensor([0.9955, 1.0057, 1.0091, 0.9963, 0.9967, 0.9969, 1.0063, 0.9965, 1.0080,\n",
       "                       1.0035, 0.9952, 1.0008, 1.0018, 0.9964, 0.9962, 0.9974, 0.9964, 0.9990,\n",
       "                       0.9954, 0.9962, 1.0012, 0.9937, 1.0061, 1.0056, 0.9958, 1.0042, 0.9972,\n",
       "                       1.0040, 0.9969, 1.0087, 1.0111, 1.0073, 1.0037, 0.9961, 1.0039, 0.9970,\n",
       "                       1.0058, 0.9962, 1.0017, 0.9971, 1.0066, 1.0069, 1.0057, 0.9968, 1.0134,\n",
       "                       0.9961, 0.9964, 1.0040, 1.0056, 1.0074, 1.0053, 0.9969, 0.9955, 0.9961,\n",
       "                       1.0062, 0.9969, 0.9986, 1.0053, 0.9984, 0.9986, 1.0039, 1.0049, 0.9951,\n",
       "                       0.9969], device='cuda:0')),\n",
       "              ('transformer.resblocks.2.ln_1.bias',\n",
       "               tensor([-0.0046,  0.0042,  0.0028,  0.0040,  0.0056, -0.0102,  0.0018, -0.0046,\n",
       "                       -0.0051,  0.0033,  0.0065, -0.0042,  0.0081,  0.0037,  0.0061,  0.0008,\n",
       "                       -0.0020, -0.0052, -0.0044, -0.0011, -0.0061,  0.0046, -0.0055, -0.0058,\n",
       "                       -0.0049,  0.0056,  0.0049, -0.0056, -0.0044, -0.0002, -0.0055,  0.0054,\n",
       "                       -0.0044,  0.0043, -0.0097,  0.0046, -0.0049,  0.0048,  0.0130,  0.0041,\n",
       "                       -0.0040, -0.0054, -0.0051,  0.0031,  0.0050,  0.0050, -0.0071, -0.0039,\n",
       "                        0.0019, -0.0050,  0.0061,  0.0047, -0.0045, -0.0054,  0.0053, -0.0057,\n",
       "                       -0.0048, -0.0057, -0.0038,  0.0066,  0.0009, -0.0049,  0.0052, -0.0047],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.2.ln_2.weight',\n",
       "               tensor([1.0051, 0.9972, 1.0073, 1.0084, 1.0006, 1.0127, 1.0099, 1.0053, 1.0064,\n",
       "                       1.0077, 1.0013, 0.9958, 1.0051, 0.9949, 1.0022, 1.0127, 1.0077, 0.9961,\n",
       "                       1.0002, 1.0056, 1.0073, 0.9958, 1.0054, 1.0067, 1.0089, 1.0080, 1.0123,\n",
       "                       0.9956, 0.9960, 1.0058, 1.0094, 1.0053, 0.9953, 0.9974, 1.0118, 0.9975,\n",
       "                       0.9990, 0.9983, 1.0067, 0.9953, 0.9956, 0.9977, 0.9999, 1.0064, 0.9993,\n",
       "                       0.9914, 1.0067, 1.0091, 1.0059, 1.0125, 0.9963, 0.9993, 1.0065, 0.9984,\n",
       "                       1.0083, 1.0086, 1.0025, 0.9957, 1.0051, 0.9955, 0.9954, 0.9929, 0.9965,\n",
       "                       1.0047], device='cuda:0')),\n",
       "              ('transformer.resblocks.2.ln_2.bias',\n",
       "               tensor([ 0.0066, -0.0012,  0.0005, -0.0064,  0.0033, -0.0060, -0.0058, -0.0051,\n",
       "                        0.0057,  0.0094, -0.0048, -0.0037,  0.0077,  0.0041, -0.0047, -0.0063,\n",
       "                       -0.0059, -0.0048, -0.0042, -0.0051,  0.0069,  0.0046,  0.0053,  0.0061,\n",
       "                        0.0056,  0.0072,  0.0069, -0.0043, -0.0043,  0.0057, -0.0065,  0.0057,\n",
       "                       -0.0045,  0.0024,  0.0088, -0.0089, -0.0041,  0.0044, -0.0062, -0.0004,\n",
       "                        0.0053, -0.0068, -0.0080,  0.0053,  0.0018,  0.0059,  0.0101,  0.0067,\n",
       "                       -0.0037,  0.0103, -0.0063, -0.0030,  0.0048,  0.0015,  0.0075, -0.0054,\n",
       "                       -0.0028, -0.0040,  0.0049, -0.0039,  0.0043,  0.0056, -0.0052,  0.0007],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.attn.in_proj_weight',\n",
       "               tensor([[-0.0755,  0.0512, -0.0278,  ...,  0.0637,  0.0447, -0.0272],\n",
       "                       [-0.1274,  0.1487, -0.0812,  ...,  0.0226, -0.0275,  0.0622],\n",
       "                       [-0.0954, -0.0367,  0.1525,  ...,  0.0167,  0.0079, -0.0046],\n",
       "                       ...,\n",
       "                       [-0.0317,  0.0444,  0.1314,  ...,  0.0624,  0.0835, -0.0760],\n",
       "                       [-0.1340,  0.0134, -0.0373,  ..., -0.0201,  0.0740, -0.1082],\n",
       "                       [ 0.0983, -0.1050,  0.0057,  ...,  0.1175,  0.1437, -0.0953]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.attn.in_proj_bias',\n",
       "               tensor([-4.8579e-03,  6.3751e-04, -5.1961e-03, -5.5514e-03,  4.9400e-03,\n",
       "                       -4.4765e-03, -1.3692e-03, -4.9985e-03,  4.5660e-03,  3.9983e-03,\n",
       "                        6.0845e-03, -6.6118e-03,  5.5439e-03,  6.3397e-03, -4.9711e-03,\n",
       "                        8.0136e-03,  4.8500e-03,  4.5362e-03, -5.2341e-03,  4.7261e-03,\n",
       "                       -1.1866e-03,  4.3905e-03, -6.0731e-03,  6.6977e-03,  5.8948e-03,\n",
       "                        5.2608e-03,  5.0897e-03, -2.8918e-03,  3.8605e-03, -4.9783e-03,\n",
       "                       -1.8803e-03, -5.0168e-03, -1.3778e-03,  4.1442e-03, -4.3777e-03,\n",
       "                       -2.4513e-03,  1.4003e-03, -4.6640e-03, -3.7839e-03, -1.9313e-03,\n",
       "                        4.8097e-03,  3.8541e-03, -4.5908e-03, -3.6567e-03,  1.4237e-03,\n",
       "                       -5.4093e-03, -5.8354e-03,  5.2743e-03,  5.2156e-03,  4.4174e-03,\n",
       "                       -4.4546e-03, -5.3412e-03,  3.9601e-03,  4.8859e-03,  5.8931e-03,\n",
       "                        6.1383e-03,  4.8902e-03, -4.2488e-03,  4.0188e-03,  4.3005e-03,\n",
       "                       -4.9246e-03,  4.9600e-03, -5.7446e-03,  4.8664e-03, -3.5307e-06,\n",
       "                       -3.3476e-06, -6.8045e-06, -4.4635e-06, -1.3255e-06, -7.9278e-06,\n",
       "                       -5.2190e-06,  7.1399e-07,  3.6364e-06,  1.0412e-06,  2.0140e-06,\n",
       "                        1.2370e-06, -3.8688e-06, -3.0824e-05, -7.4586e-06, -1.8295e-07,\n",
       "                       -8.7437e-06, -1.0989e-05, -5.2579e-06,  1.0080e-05,  1.1358e-05,\n",
       "                       -9.2523e-07,  3.7786e-06, -3.8288e-06, -1.3349e-05, -9.6799e-06,\n",
       "                        2.1999e-06, -2.1012e-06, -3.3065e-06,  1.4556e-06,  1.9752e-06,\n",
       "                       -3.8545e-06, -1.5988e-05, -5.9953e-06, -1.9023e-06, -2.4812e-06,\n",
       "                        1.6141e-05, -5.8819e-06,  1.9063e-06,  1.2836e-08, -7.8671e-07,\n",
       "                        4.7678e-07, -1.5878e-06, -3.2626e-06,  3.8889e-06, -8.8131e-06,\n",
       "                       -1.6382e-05,  2.5472e-07, -3.0576e-06, -8.1409e-06, -1.3302e-05,\n",
       "                        6.2674e-07, -9.3786e-07,  1.3197e-06,  1.1261e-05, -1.7599e-05,\n",
       "                        1.1577e-06,  1.2082e-05, -5.2530e-06, -4.9274e-06,  1.2018e-06,\n",
       "                       -1.0983e-05,  2.2045e-05, -4.7727e-06, -6.8599e-03,  4.7543e-03,\n",
       "                        3.7998e-03, -5.1802e-03,  4.0999e-03,  7.6158e-03, -4.2703e-03,\n",
       "                        5.5546e-03, -4.4533e-03,  6.0978e-03,  1.1614e-02, -4.8186e-03,\n",
       "                       -4.7552e-03,  6.7051e-03, -4.9710e-03,  8.5540e-03,  4.6549e-03,\n",
       "                        4.6928e-03,  5.1661e-03,  4.3454e-03, -5.2449e-03, -5.1694e-03,\n",
       "                        5.4570e-03,  4.2318e-03,  2.0992e-03, -4.9706e-03,  5.1230e-03,\n",
       "                        1.1024e-03,  4.9489e-03,  5.6095e-03,  4.8784e-03, -4.9783e-03,\n",
       "                        5.1034e-03, -5.6446e-03,  4.1238e-03,  4.0522e-03, -8.7986e-03,\n",
       "                       -4.5238e-03, -5.7764e-03,  1.6926e-03, -3.6707e-03,  4.3337e-03,\n",
       "                        5.2148e-03, -4.4775e-03,  4.9414e-03, -8.6213e-03, -4.9856e-03,\n",
       "                        5.6751e-03,  4.4865e-03, -5.1289e-03,  5.8940e-03,  4.8605e-03,\n",
       "                        1.3857e-03, -5.4079e-03, -4.3790e-03, -4.9263e-03,  3.1079e-03,\n",
       "                        5.0422e-03,  4.6794e-03,  4.9721e-03, -7.0310e-03, -4.6344e-03,\n",
       "                        4.9452e-03,  4.4634e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.3.attn.out_proj.weight',\n",
       "               tensor([[-0.0387, -0.0256,  0.0209,  ...,  0.1111,  0.0282,  0.0556],\n",
       "                       [-0.0257,  0.0988, -0.1167,  ..., -0.1004, -0.0187,  0.0874],\n",
       "                       [-0.1084, -0.0331,  0.1133,  ..., -0.0514,  0.0315,  0.0545],\n",
       "                       ...,\n",
       "                       [ 0.0516,  0.0669,  0.0881,  ...,  0.1123, -0.0951,  0.0688],\n",
       "                       [-0.0462, -0.0433, -0.1132,  ...,  0.1079, -0.0524,  0.0323],\n",
       "                       [ 0.0025, -0.1054, -0.0756,  ..., -0.0212,  0.0859, -0.0281]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.attn.out_proj.bias',\n",
       "               tensor([ 0.0050, -0.0036, -0.0043, -0.0051, -0.0053, -0.0050, -0.0053, -0.0070,\n",
       "                        0.0049,  0.0045, -0.0051,  0.0041,  0.0048,  0.0048, -0.0048, -0.0055,\n",
       "                       -0.0057, -0.0047,  0.0055, -0.0046, -0.0026, -0.0054,  0.0049,  0.0007,\n",
       "                        0.0055,  0.0049,  0.0048,  0.0049, -0.0048,  0.0051, -0.0048,  0.0048,\n",
       "                        0.0052,  0.0043,  0.0054,  0.0046, -0.0050,  0.0030, -0.0050,  0.0053,\n",
       "                       -0.0046, -0.0052, -0.0048, -0.0045, -0.0054, -0.0050,  0.0050,  0.0051,\n",
       "                        0.0050,  0.0045,  0.0050,  0.0050, -0.0028, -0.0052,  0.0053,  0.0045,\n",
       "                        0.0060,  0.0052, -0.0042,  0.0041, -0.0048, -0.0068, -0.0054, -0.0045],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.mlp.0.weight',\n",
       "               tensor([[-0.0164, -0.0646, -0.0247,  ..., -0.0588,  0.0413,  0.0549],\n",
       "                       [ 0.0436,  0.0768, -0.0840,  ...,  0.0218,  0.0650,  0.0423],\n",
       "                       [-0.0072, -0.0177,  0.1114,  ...,  0.1092, -0.0867,  0.1339],\n",
       "                       ...,\n",
       "                       [ 0.0222, -0.0656, -0.1118,  ...,  0.0300,  0.0464, -0.0158],\n",
       "                       [-0.0437, -0.0927, -0.0171,  ..., -0.0378, -0.0047, -0.1191],\n",
       "                       [-0.1005,  0.0812,  0.1124,  ...,  0.0496, -0.0575,  0.1132]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.mlp.0.bias',\n",
       "               tensor([-0.0633,  0.0832,  0.0245,  0.0449, -0.0424, -0.0909,  0.0753,  0.1133,\n",
       "                        0.0265,  0.0248, -0.0299, -0.0684,  0.0565,  0.0196,  0.1285, -0.0684,\n",
       "                       -0.1151,  0.0754, -0.0532,  0.0225,  0.0742, -0.1159, -0.0712, -0.0668,\n",
       "                        0.0430, -0.0498,  0.0287, -0.0373, -0.0447,  0.0432,  0.0854,  0.0123,\n",
       "                        0.0654, -0.0502, -0.0951, -0.0046,  0.0270, -0.0445,  0.0102, -0.0728,\n",
       "                       -0.1063,  0.0872, -0.0353, -0.0943, -0.0325, -0.0851, -0.0821, -0.0361,\n",
       "                       -0.0611, -0.0358,  0.0080,  0.0302, -0.0739,  0.1088, -0.0011, -0.0049,\n",
       "                       -0.0112, -0.0852,  0.0200, -0.0982,  0.0194, -0.1178,  0.0978, -0.0559,\n",
       "                       -0.0975,  0.0598, -0.0436,  0.0010, -0.1135, -0.0693, -0.0833,  0.0282,\n",
       "                       -0.0641,  0.1022,  0.1007,  0.0787,  0.0243,  0.0498, -0.0069, -0.0573,\n",
       "                       -0.1048, -0.0182, -0.0172, -0.0693,  0.0926,  0.0119, -0.1038, -0.1253,\n",
       "                       -0.0446,  0.0902,  0.0456,  0.0267, -0.0193, -0.0292,  0.0905,  0.0803,\n",
       "                       -0.1012,  0.0280, -0.0908,  0.0143, -0.0401,  0.0771,  0.0576,  0.1086,\n",
       "                       -0.0122, -0.0727, -0.0920, -0.0453,  0.1104,  0.0865, -0.0909,  0.0033,\n",
       "                       -0.1021,  0.1019,  0.0806, -0.1128, -0.0304,  0.0875,  0.0188, -0.0646,\n",
       "                        0.1015, -0.0942, -0.0105,  0.0302, -0.0836, -0.1285, -0.0489, -0.0774,\n",
       "                       -0.1163,  0.0929, -0.0657,  0.0098,  0.0664, -0.0654, -0.0404,  0.0739,\n",
       "                       -0.0496, -0.0265, -0.0557, -0.0169,  0.0023, -0.0947,  0.0106, -0.0530,\n",
       "                        0.0384,  0.0375,  0.1000,  0.0901,  0.0153,  0.0781, -0.1085, -0.0770,\n",
       "                        0.0190,  0.1247, -0.0551, -0.0253, -0.0689, -0.0560,  0.0823,  0.0038,\n",
       "                        0.0490,  0.0588,  0.0608,  0.0371,  0.0052, -0.1001,  0.0965,  0.1210,\n",
       "                        0.0401, -0.0209,  0.0366, -0.0294, -0.1036,  0.0245,  0.0504,  0.0187,\n",
       "                       -0.0018,  0.0744,  0.1080, -0.0622, -0.0240,  0.0307,  0.0995,  0.1119,\n",
       "                        0.0438,  0.0149, -0.1151,  0.0432,  0.0122, -0.0616,  0.0872, -0.1123,\n",
       "                        0.0393,  0.0019,  0.1046,  0.1058,  0.0114,  0.1003,  0.1115,  0.1051,\n",
       "                       -0.0834,  0.0510,  0.1123,  0.1210, -0.0187, -0.1031, -0.0734, -0.0748,\n",
       "                        0.1122, -0.0213, -0.0835,  0.1287,  0.0129, -0.0415,  0.0196, -0.0224,\n",
       "                       -0.0086,  0.0907,  0.0278, -0.0106,  0.0624,  0.1224, -0.0807,  0.0189,\n",
       "                       -0.0575,  0.0362, -0.0330, -0.0064, -0.0492,  0.0923, -0.1069,  0.0650,\n",
       "                       -0.0237,  0.0595,  0.0938,  0.0043, -0.1014,  0.1078,  0.0753,  0.0301,\n",
       "                        0.0078, -0.0080,  0.0918,  0.0418,  0.0380, -0.0441,  0.0419, -0.0479,\n",
       "                        0.0991, -0.0743, -0.0216, -0.0081,  0.0485,  0.1157, -0.0340, -0.0873],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.mlp.2.weight',\n",
       "               tensor([[ 0.0292,  0.0207, -0.0203,  ..., -0.0508,  0.0092,  0.0213],\n",
       "                       [ 0.0033, -0.0530,  0.0153,  ...,  0.0097,  0.0604,  0.0442],\n",
       "                       [-0.0312,  0.0355,  0.0409,  ..., -0.0626,  0.0529,  0.0168],\n",
       "                       ...,\n",
       "                       [-0.0243,  0.0410,  0.0193,  ...,  0.0217, -0.0564,  0.0207],\n",
       "                       [-0.0429, -0.0169,  0.0236,  ...,  0.0450, -0.0127,  0.0472],\n",
       "                       [-0.0590,  0.0477,  0.0235,  ...,  0.0570,  0.0290,  0.0539]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.mlp.2.bias',\n",
       "               tensor([ 0.0595, -0.0276, -0.0625, -0.0457,  0.0565, -0.0045, -0.0647, -0.0565,\n",
       "                        0.0505,  0.0338, -0.0503, -0.0479,  0.0323, -0.0299,  0.0235,  0.0539,\n",
       "                       -0.0539,  0.0227,  0.0616, -0.0089, -0.0299,  0.0269, -0.0436,  0.0509,\n",
       "                        0.0540, -0.0472,  0.0543, -0.0346, -0.0081,  0.0271, -0.0316,  0.0402,\n",
       "                        0.0058, -0.0161, -0.0560,  0.0362, -0.0281, -0.0559, -0.0502,  0.0665,\n",
       "                        0.0312,  0.0206, -0.0386,  0.0014,  0.0321,  0.0032,  0.0341,  0.0587,\n",
       "                        0.0526, -0.0478,  0.0026,  0.0660, -0.0005, -0.0371,  0.0524, -0.0535,\n",
       "                       -0.0346,  0.0548, -0.0479,  0.0417, -0.0491,  0.0400,  0.0229,  0.0492],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.ln_1.weight',\n",
       "               tensor([0.9959, 0.9969, 0.9989, 1.0081, 1.0046, 1.0088, 1.0075, 0.9958, 0.9976,\n",
       "                       0.9891, 1.0058, 0.9963, 1.0046, 1.0045, 1.0012, 1.0077, 1.0052, 1.0038,\n",
       "                       1.0082, 0.9958, 0.9961, 1.0066, 0.9981, 0.9950, 0.9950, 1.0044, 1.0116,\n",
       "                       1.0067, 1.0056, 1.0062, 1.0072, 0.9946, 0.9945, 0.9952, 1.0042, 1.0013,\n",
       "                       1.0040, 0.9965, 0.9974, 0.9973, 0.9984, 1.0087, 1.0016, 0.9972, 1.0013,\n",
       "                       0.9951, 0.9939, 1.0059, 1.0042, 1.0010, 0.9963, 1.0080, 0.9992, 0.9964,\n",
       "                       0.9949, 1.0037, 0.9982, 1.0007, 0.9965, 1.0033, 1.0026, 0.9946, 1.0040,\n",
       "                       0.9938], device='cuda:0')),\n",
       "              ('transformer.resblocks.3.ln_1.bias',\n",
       "               tensor([ 0.0052, -0.0039, -0.0035, -0.0116,  0.0052, -0.0066, -0.0085, -0.0050,\n",
       "                       -0.0052, -0.0051, -0.0060, -0.0049, -0.0067, -0.0051,  0.0075, -0.0091,\n",
       "                       -0.0044, -0.0047,  0.0061,  0.0011, -0.0037, -0.0063, -0.0051, -0.0028,\n",
       "                       -0.0048, -0.0034,  0.0058, -0.0003,  0.0055,  0.0066,  0.0037, -0.0048,\n",
       "                       -0.0051,  0.0041,  0.0027,  0.0053,  0.0041, -0.0062,  0.0006,  0.0048,\n",
       "                        0.0054, -0.0045,  0.0053,  0.0053,  0.0046, -0.0047, -0.0051,  0.0047,\n",
       "                       -0.0048,  0.0066,  0.0069,  0.0049,  0.0057, -0.0048, -0.0048,  0.0013,\n",
       "                       -0.0051, -0.0057, -0.0050,  0.0091, -0.0050,  0.0048,  0.0061,  0.0080],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.3.ln_2.weight',\n",
       "               tensor([1.0061, 0.9952, 1.0105, 0.9958, 1.0070, 1.0100, 1.0010, 1.0099, 1.0122,\n",
       "                       1.0051, 1.0066, 1.0046, 1.0083, 0.9972, 1.0043, 0.9952, 1.0075, 1.0036,\n",
       "                       1.0089, 0.9980, 0.9975, 0.9991, 1.0076, 0.9997, 1.0061, 1.0031, 1.0023,\n",
       "                       1.0102, 0.9960, 1.0079, 1.0068, 1.0064, 1.0058, 0.9962, 1.0064, 1.0050,\n",
       "                       1.0053, 1.0053, 1.0053, 0.9974, 1.0055, 1.0081, 1.0065, 1.0075, 1.0081,\n",
       "                       0.9972, 0.9964, 1.0100, 0.9962, 1.0088, 0.9964, 0.9960, 0.9965, 1.0071,\n",
       "                       1.0070, 0.9992, 1.0046, 1.0110, 0.9979, 1.0060, 1.0058, 1.0084, 1.0097,\n",
       "                       1.0064], device='cuda:0')),\n",
       "              ('transformer.resblocks.3.ln_2.bias',\n",
       "               tensor([ 0.0060,  0.0057, -0.0122,  0.0039, -0.0008, -0.0015, -0.0074, -0.0099,\n",
       "                        0.0067,  0.0044, -0.0062,  0.0049,  0.0063,  0.0055,  0.0058,  0.0046,\n",
       "                       -0.0088, -0.0053,  0.0051,  0.0038, -0.0032,  0.0003,  0.0057, -0.0047,\n",
       "                        0.0060, -0.0002,  0.0064,  0.0083, -0.0042,  0.0080, -0.0066,  0.0056,\n",
       "                        0.0056,  0.0048,  0.0096, -0.0077, -0.0144, -0.0104, -0.0049,  0.0061,\n",
       "                       -0.0091, -0.0054, -0.0058, -0.0053, -0.0060, -0.0051, -0.0050,  0.0063,\n",
       "                        0.0073,  0.0069,  0.0040, -0.0008, -0.0048, -0.0066,  0.0062, -0.0011,\n",
       "                        0.0071,  0.0103,  0.0061,  0.0078, -0.0068, -0.0114, -0.0071, -0.0051],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.attn.in_proj_weight',\n",
       "               tensor([[ 0.1150,  0.0423,  0.0245,  ..., -0.0158,  0.0087,  0.0256],\n",
       "                       [ 0.1231, -0.0005,  0.0107,  ..., -0.1134,  0.0904,  0.0526],\n",
       "                       [-0.0262,  0.0594,  0.1038,  ..., -0.0370, -0.1397, -0.0126],\n",
       "                       ...,\n",
       "                       [-0.0493, -0.0981, -0.0197,  ..., -0.0818, -0.0304, -0.0949],\n",
       "                       [ 0.0773, -0.0641, -0.0238,  ...,  0.0329, -0.1069,  0.0777],\n",
       "                       [-0.0477, -0.1089,  0.1270,  ...,  0.1175,  0.1196,  0.0678]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.attn.in_proj_bias',\n",
       "               tensor([-4.4292e-03,  5.1587e-03, -4.8175e-03, -4.8075e-03,  4.9276e-03,\n",
       "                       -6.5130e-03, -3.9218e-03, -4.6012e-03,  5.9949e-03, -8.6046e-03,\n",
       "                       -5.7962e-03, -3.6364e-03,  3.2191e-03,  7.5024e-03,  5.9151e-03,\n",
       "                        2.7010e-03,  1.5880e-03,  1.0073e-03, -8.0350e-03, -5.4298e-03,\n",
       "                        6.9846e-04,  5.6033e-03, -7.2314e-04,  4.4829e-03,  4.5780e-03,\n",
       "                        5.9081e-03, -4.1433e-03,  4.5901e-03,  4.9084e-03, -3.8201e-03,\n",
       "                       -5.0735e-03, -4.6783e-03, -5.2524e-03, -5.9290e-03, -4.4693e-03,\n",
       "                        4.5740e-03, -7.4349e-03,  5.6032e-03, -7.8703e-03,  4.9874e-03,\n",
       "                       -3.5709e-03,  7.0360e-03,  6.3332e-03, -4.8637e-03,  3.9764e-03,\n",
       "                       -5.1655e-03,  8.5046e-03, -5.0337e-03, -5.1682e-03, -5.9079e-03,\n",
       "                       -5.2647e-03,  4.7001e-03, -7.0218e-03,  6.1712e-03, -5.5004e-03,\n",
       "                        6.8143e-03, -4.3040e-03,  3.8279e-03,  2.6525e-03,  5.2373e-03,\n",
       "                        4.4378e-03, -4.6116e-03, -5.7740e-03,  4.9087e-03,  7.9454e-06,\n",
       "                        6.9054e-07,  1.2711e-05,  5.4288e-06, -2.0267e-06, -2.3015e-06,\n",
       "                       -8.4664e-06,  3.8904e-06, -1.4747e-05,  1.1540e-06,  4.4730e-07,\n",
       "                       -5.3059e-06,  5.2007e-06, -1.7845e-06, -6.3819e-06, -2.1729e-06,\n",
       "                       -7.7019e-07,  4.4926e-06, -6.2767e-06,  8.2903e-06,  6.1031e-06,\n",
       "                        5.0980e-06, -5.4811e-06, -6.1130e-06, -1.9626e-05, -1.0868e-05,\n",
       "                        2.0688e-06, -3.7031e-06, -1.2133e-05, -1.6171e-06,  1.3641e-05,\n",
       "                       -8.7831e-06, -2.3619e-06,  5.5300e-06,  3.1698e-07,  3.6427e-06,\n",
       "                        5.1286e-06,  4.8228e-06, -3.9203e-06,  7.6682e-06, -9.7080e-06,\n",
       "                       -7.0746e-07, -1.8842e-05, -2.0833e-06,  2.1595e-06,  3.9575e-06,\n",
       "                        9.9209e-07, -1.5119e-05,  4.3595e-06,  2.9953e-06, -2.4144e-06,\n",
       "                       -5.4552e-06,  1.9738e-06,  7.3850e-06, -7.0832e-06,  1.3554e-05,\n",
       "                        1.8377e-06,  2.9724e-06, -6.1614e-07,  1.5225e-05, -1.4557e-05,\n",
       "                        1.7908e-06,  1.5013e-07, -6.7783e-07,  5.0046e-03, -5.0563e-03,\n",
       "                        5.7549e-03, -5.9088e-03, -5.7538e-03,  5.4653e-03,  4.9099e-03,\n",
       "                        4.9911e-03,  5.4405e-03,  6.3952e-03,  2.7363e-03, -5.4429e-03,\n",
       "                        4.5734e-03,  4.5188e-03, -6.0399e-03,  4.6363e-03, -6.7311e-03,\n",
       "                        3.9778e-03, -5.1240e-03,  4.2764e-03,  5.8195e-03, -5.5397e-03,\n",
       "                       -4.9628e-03, -6.0169e-03, -5.1200e-03,  7.2287e-03, -4.8999e-03,\n",
       "                       -4.7995e-03, -5.1331e-03,  4.7698e-03, -5.5478e-03, -1.0057e-02,\n",
       "                        4.5150e-03, -4.8053e-03, -4.2165e-03, -5.1973e-03, -5.0659e-03,\n",
       "                       -4.1603e-03, -5.3950e-03,  3.2023e-03,  7.9594e-03,  4.9696e-03,\n",
       "                       -5.3201e-03,  4.9872e-03, -5.1489e-03, -4.7004e-03,  5.0362e-03,\n",
       "                        4.9873e-03, -4.4328e-03, -4.6024e-03,  4.5469e-03, -6.5897e-03,\n",
       "                        5.3720e-03, -5.5300e-03,  4.0257e-03,  5.6019e-03,  2.0594e-03,\n",
       "                        4.7907e-03, -5.2829e-03, -3.3983e-03, -5.2529e-03,  5.1661e-03,\n",
       "                       -4.7353e-03,  1.0450e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.4.attn.out_proj.weight',\n",
       "               tensor([[-0.0977,  0.0206,  0.0836,  ...,  0.0806,  0.0524, -0.0873],\n",
       "                       [ 0.0279, -0.0574,  0.1109,  ...,  0.0903, -0.0744,  0.1221],\n",
       "                       [ 0.0097, -0.1090, -0.0374,  ..., -0.1109,  0.0227, -0.0308],\n",
       "                       ...,\n",
       "                       [ 0.1047,  0.0702, -0.0566,  ..., -0.1177,  0.0852,  0.0954],\n",
       "                       [-0.0394, -0.0917, -0.1255,  ..., -0.0182,  0.0935, -0.0485],\n",
       "                       [-0.0882, -0.0674, -0.0414,  ..., -0.0218,  0.0811, -0.0998]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.attn.out_proj.bias',\n",
       "               tensor([ 0.0052,  0.0018, -0.0043, -0.0050, -0.0051, -0.0051, -0.0051, -0.0053,\n",
       "                        0.0047,  0.0049, -0.0052, -0.0048,  0.0047,  0.0050, -0.0056, -0.0055,\n",
       "                       -0.0052, -0.0046,  0.0056, -0.0049,  0.0028, -0.0042,  0.0051,  0.0003,\n",
       "                        0.0060,  0.0047,  0.0049,  0.0049, -0.0060,  0.0053, -0.0046,  0.0050,\n",
       "                        0.0052,  0.0050,  0.0052,  0.0052, -0.0051, -0.0056, -0.0052,  0.0054,\n",
       "                       -0.0049, -0.0054, -0.0049,  0.0024, -0.0057, -0.0053,  0.0052,  0.0050,\n",
       "                        0.0052,  0.0048,  0.0049,  0.0051,  0.0051, -0.0053,  0.0055,  0.0022,\n",
       "                       -0.0051,  0.0051,  0.0083,  0.0046, -0.0048, -0.0074, -0.0053, -0.0047],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.mlp.0.weight',\n",
       "               tensor([[-0.0169,  0.0080, -0.0706,  ...,  0.0337, -0.0224,  0.0656],\n",
       "                       [ 0.0468,  0.1008,  0.1172,  ...,  0.0160, -0.0745, -0.0779],\n",
       "                       [-0.0342, -0.0820,  0.0645,  ...,  0.0147, -0.0511, -0.0755],\n",
       "                       ...,\n",
       "                       [ 0.1187,  0.0498,  0.0046,  ...,  0.0379,  0.0522, -0.0200],\n",
       "                       [ 0.0199,  0.0949,  0.0236,  ..., -0.0782,  0.0426,  0.0198],\n",
       "                       [-0.1235,  0.1029,  0.0643,  ...,  0.1116, -0.0447,  0.0911]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.mlp.0.bias',\n",
       "               tensor([ 0.0297, -0.0815,  0.0596, -0.0121,  0.0010, -0.0874, -0.0241,  0.0729,\n",
       "                       -0.0921,  0.1115, -0.0448,  0.0044, -0.0471, -0.0846, -0.0383,  0.0596,\n",
       "                       -0.0478,  0.0785,  0.1191,  0.0272,  0.0498, -0.0090,  0.0215, -0.0972,\n",
       "                       -0.0498,  0.0409,  0.0247, -0.1079,  0.0413,  0.0584, -0.0578,  0.0210,\n",
       "                       -0.0880,  0.0848, -0.0771,  0.0902,  0.0434, -0.0460,  0.0484,  0.0638,\n",
       "                       -0.0163, -0.0022,  0.0474,  0.0757,  0.0732, -0.1129,  0.0772,  0.1232,\n",
       "                       -0.1058, -0.0183, -0.1083,  0.1238, -0.1088, -0.1038, -0.0975, -0.0865,\n",
       "                        0.0555, -0.0409,  0.1004, -0.0392, -0.0314,  0.1122, -0.0477,  0.0837,\n",
       "                        0.0221, -0.0013,  0.0253, -0.0239, -0.0049, -0.1132, -0.0060, -0.0005,\n",
       "                       -0.0269, -0.1071, -0.1168,  0.0940, -0.0565, -0.0200, -0.0348,  0.0300,\n",
       "                       -0.0347, -0.0438, -0.0604, -0.0399, -0.1172,  0.0052, -0.1241,  0.1147,\n",
       "                       -0.0552, -0.1060,  0.0263,  0.0110, -0.1065, -0.0272, -0.0445, -0.0615,\n",
       "                        0.0568,  0.0078,  0.0546, -0.0745, -0.0517, -0.0742, -0.1065, -0.0926,\n",
       "                       -0.1203,  0.0983,  0.0204, -0.0988, -0.0732, -0.0230, -0.0493,  0.0913,\n",
       "                       -0.1066, -0.0156,  0.0506,  0.0441,  0.0448, -0.0625,  0.0610,  0.0695,\n",
       "                        0.0129,  0.0582,  0.0931, -0.0111, -0.0568,  0.0785, -0.0706, -0.0841,\n",
       "                       -0.0199, -0.0811, -0.0175, -0.0499, -0.0958, -0.0078, -0.0630, -0.0945,\n",
       "                       -0.0881, -0.0502, -0.0516, -0.0222, -0.0700, -0.0634,  0.1002,  0.0099,\n",
       "                        0.0007, -0.0889,  0.0153, -0.0700,  0.1264,  0.0536, -0.0459, -0.0259,\n",
       "                       -0.0886, -0.0033, -0.0107,  0.0192,  0.0959,  0.0986, -0.0307,  0.0990,\n",
       "                        0.0060, -0.0110, -0.0447,  0.0222,  0.0055,  0.1043,  0.1036,  0.0553,\n",
       "                        0.0255, -0.1078,  0.0225, -0.0362,  0.0060,  0.0801,  0.0870, -0.0807,\n",
       "                       -0.1145,  0.0820, -0.0830, -0.1033,  0.0680, -0.0811,  0.0608, -0.1144,\n",
       "                       -0.0381,  0.1121, -0.1046,  0.0109,  0.0985,  0.0276, -0.1211,  0.0973,\n",
       "                        0.0446,  0.0935, -0.1288,  0.1097,  0.0891, -0.0683, -0.1035, -0.0695,\n",
       "                        0.0594,  0.1226,  0.0833, -0.0958, -0.1217,  0.0147,  0.0272, -0.0448,\n",
       "                       -0.0409,  0.0167,  0.0279,  0.0580, -0.0427,  0.0232,  0.0177, -0.0398,\n",
       "                       -0.0773, -0.0433,  0.0513, -0.0652,  0.0510,  0.0732, -0.0466, -0.0563,\n",
       "                        0.0578,  0.0444, -0.0264,  0.0432, -0.0158, -0.0055,  0.0769,  0.0869,\n",
       "                        0.0455, -0.0853,  0.0789, -0.0503, -0.0202,  0.0287, -0.0019, -0.0859,\n",
       "                        0.0427,  0.0895,  0.0633,  0.0496,  0.1110,  0.1045, -0.0607, -0.0800,\n",
       "                       -0.0423, -0.0805,  0.1041, -0.0260,  0.0391,  0.1173, -0.0210, -0.0861],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.mlp.2.weight',\n",
       "               tensor([[-0.0192, -0.0346,  0.0538,  ..., -0.0091, -0.0453, -0.0012],\n",
       "                       [-0.0413,  0.0493,  0.0085,  ..., -0.0041,  0.0082,  0.0219],\n",
       "                       [-0.0539,  0.0413,  0.0229,  ..., -0.0077, -0.0632, -0.0131],\n",
       "                       ...,\n",
       "                       [-0.0400, -0.0397, -0.0084,  ...,  0.0576,  0.0577, -0.0381],\n",
       "                       [ 0.0342,  0.0427, -0.0185,  ...,  0.0056,  0.0011,  0.0120],\n",
       "                       [-0.0165, -0.0511, -0.0149,  ...,  0.0186,  0.0062,  0.0277]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.mlp.2.bias',\n",
       "               tensor([-0.0289, -0.0483,  0.0269,  0.0507,  0.0451,  0.0080,  0.0487, -0.0029,\n",
       "                       -0.0321, -0.0493,  0.0442,  0.0208,  0.0616, -0.0514, -0.0635, -0.0139,\n",
       "                        0.0266,  0.0431,  0.0129, -0.0351, -0.0097, -0.0459, -0.0412, -0.0045,\n",
       "                        0.0336,  0.0664, -0.0107,  0.0375, -0.0522, -0.0353,  0.0148,  0.0473,\n",
       "                        0.0549,  0.0077, -0.0096,  0.0627,  0.0349,  0.0233, -0.0514,  0.0042,\n",
       "                        0.0057, -0.0392,  0.0183, -0.0415, -0.0394, -0.0566, -0.0381,  0.0258,\n",
       "                       -0.0238,  0.0363, -0.0050,  0.0333, -0.0404,  0.0092, -0.0451,  0.0610,\n",
       "                        0.0333, -0.0240,  0.0372,  0.0385, -0.0339,  0.0147, -0.0289,  0.0374],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.ln_1.weight',\n",
       "               tensor([1.0126, 1.0041, 0.9956, 1.0070, 1.0054, 0.9957, 1.0062, 1.0030, 0.9962,\n",
       "                       1.0028, 1.0105, 0.9981, 1.0001, 1.0005, 1.0043, 0.9985, 0.9970, 0.9999,\n",
       "                       0.9982, 0.9962, 0.9961, 1.0064, 1.0036, 0.9959, 1.0057, 0.9982, 1.0058,\n",
       "                       0.9928, 0.9956, 1.0067, 1.0058, 0.9946, 1.0031, 0.9972, 1.0050, 0.9944,\n",
       "                       1.0063, 0.9953, 0.9969, 1.0059, 1.0081, 1.0070, 1.0022, 0.9990, 1.0042,\n",
       "                       1.0007, 1.0092, 1.0022, 1.0074, 1.0061, 1.0103, 0.9973, 1.0058, 0.9960,\n",
       "                       1.0063, 1.0082, 1.0060, 0.9967, 1.0091, 0.9961, 1.0066, 1.0037, 1.0043,\n",
       "                       1.0048], device='cuda:0')),\n",
       "              ('transformer.resblocks.4.ln_1.bias',\n",
       "               tensor([ 0.0065, -0.0048,  0.0043,  0.0007,  0.0042, -0.0043,  0.0039,  0.0051,\n",
       "                       -0.0023,  0.0047, -0.0056,  0.0049,  0.0058, -0.0052,  0.0059, -0.0050,\n",
       "                        0.0008, -0.0061,  0.0116, -0.0029, -0.0051, -0.0019, -0.0053,  0.0048,\n",
       "                        0.0055, -0.0005,  0.0035, -0.0050, -0.0042,  0.0052, -0.0056, -0.0052,\n",
       "                       -0.0032, -0.0111, -0.0048, -0.0058, -0.0052,  0.0048, -0.0051, -0.0048,\n",
       "                       -0.0003,  0.0034, -0.0047, -0.0054, -0.0047, -0.0051,  0.0047, -0.0027,\n",
       "                        0.0048, -0.0054,  0.0063,  0.0064, -0.0050,  0.0055,  0.0055,  0.0051,\n",
       "                        0.0057,  0.0047, -0.0052, -0.0043, -0.0055, -0.0053,  0.0051, -0.0003],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.4.ln_2.weight',\n",
       "               tensor([1.0018, 1.0040, 1.0052, 1.0011, 1.0093, 0.9958, 0.9964, 1.0104, 1.0067,\n",
       "                       1.0057, 1.0062, 1.0017, 1.0009, 1.0055, 0.9977, 1.0021, 0.9975, 0.9947,\n",
       "                       1.0116, 0.9949, 0.9964, 1.0046, 0.9997, 1.0057, 1.0059, 0.9968, 1.0077,\n",
       "                       1.0098, 0.9959, 1.0149, 1.0095, 0.9974, 1.0052, 0.9957, 1.0068, 0.9956,\n",
       "                       1.0098, 0.9953, 0.9975, 0.9986, 1.0066, 1.0139, 1.0069, 0.9985, 0.9950,\n",
       "                       1.0034, 1.0081, 1.0081, 1.0082, 1.0070, 1.0022, 1.0062, 1.0046, 1.0127,\n",
       "                       1.0081, 1.0057, 1.0051, 1.0033, 0.9954, 0.9950, 1.0055, 1.0062, 0.9996,\n",
       "                       1.0089], device='cuda:0')),\n",
       "              ('transformer.resblocks.4.ln_2.bias',\n",
       "               tensor([ 0.0142,  0.0078, -0.0053,  0.0022, -0.0085,  0.0045, -0.0008, -0.0101,\n",
       "                        0.0059,  0.0081, -0.0056,  0.0060,  0.0090,  0.0053, -0.0059,  0.0008,\n",
       "                        0.0022, -0.0038,  0.0059,  0.0048, -0.0025, -0.0053, -0.0027,  0.0055,\n",
       "                        0.0064, -0.0031,  0.0062,  0.0062, -0.0020,  0.0062, -0.0108, -0.0025,\n",
       "                        0.0065,  0.0043,  0.0060,  0.0007, -0.0068,  0.0050,  0.0031,  0.0097,\n",
       "                       -0.0075, -0.0069, -0.0063, -0.0055,  0.0038,  0.0057,  0.0063,  0.0071,\n",
       "                       -0.0006,  0.0058, -0.0060,  0.0056,  0.0070, -0.0079,  0.0060, -0.0084,\n",
       "                        0.0061,  0.0075, -0.0049, -0.0051, -0.0056,  0.0037, -0.0101, -0.0068],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.attn.in_proj_weight',\n",
       "               tensor([[-0.1018, -0.0317,  0.0844,  ..., -0.0187,  0.0271,  0.0053],\n",
       "                       [ 0.0050,  0.0665, -0.1139,  ..., -0.1021,  0.0659,  0.0030],\n",
       "                       [ 0.0084, -0.0231, -0.0564,  ..., -0.0692, -0.1142,  0.0169],\n",
       "                       ...,\n",
       "                       [-0.0737,  0.1242, -0.0250,  ...,  0.0132,  0.0886,  0.0998],\n",
       "                       [ 0.0399,  0.1018,  0.0226,  ..., -0.0312,  0.0805,  0.0344],\n",
       "                       [ 0.0519, -0.1035,  0.0215,  ..., -0.1372, -0.0842,  0.0394]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.attn.in_proj_bias',\n",
       "               tensor([-4.5075e-03,  3.9570e-03,  6.0803e-03,  4.8783e-03, -4.4973e-03,\n",
       "                       -4.2595e-03,  2.4769e-03, -4.5641e-03, -4.5927e-03,  5.1828e-03,\n",
       "                        4.5951e-03, -2.3065e-03, -4.7989e-03,  5.1610e-03,  3.8127e-03,\n",
       "                       -3.9544e-03, -4.1641e-03, -5.0589e-03,  4.3781e-03,  1.8815e-03,\n",
       "                        5.9733e-03, -5.1621e-03, -1.0404e-03,  4.7896e-03, -4.9565e-03,\n",
       "                       -4.7096e-03,  4.7715e-03,  5.2388e-03,  4.6746e-03,  4.9996e-03,\n",
       "                       -4.8877e-03,  4.9569e-03,  5.3157e-03,  5.5698e-03, -3.5841e-03,\n",
       "                       -4.5185e-03,  6.7593e-03, -5.6461e-03,  1.3864e-04, -3.8349e-03,\n",
       "                       -6.6384e-03,  4.7955e-03,  5.0886e-03,  3.9306e-03, -1.4911e-03,\n",
       "                       -4.5316e-03, -4.6791e-03, -5.0384e-03, -4.0747e-03, -3.3651e-03,\n",
       "                       -4.7919e-03,  4.5209e-03,  4.8471e-03,  3.8492e-03,  2.2798e-03,\n",
       "                       -6.9382e-03,  4.9441e-03, -3.8004e-03, -4.8415e-03,  4.9600e-03,\n",
       "                        7.3794e-03, -6.1836e-03, -3.2249e-03,  5.2064e-03,  1.0044e-05,\n",
       "                       -1.4257e-06, -1.0104e-05, -2.2328e-05,  1.6012e-06,  4.5999e-07,\n",
       "                        3.8253e-06, -1.5966e-05,  3.7243e-06,  5.4428e-06,  2.7865e-06,\n",
       "                        9.5729e-07, -4.5446e-07, -1.3348e-06,  3.1793e-06,  8.7386e-06,\n",
       "                       -4.5157e-06,  3.7101e-07,  1.0417e-06, -4.1155e-07,  1.3827e-06,\n",
       "                       -7.3310e-07,  1.4199e-06,  2.3627e-06, -3.0105e-06,  1.1564e-05,\n",
       "                       -9.2957e-06, -8.8082e-06, -4.9837e-06, -1.7151e-06,  1.5761e-05,\n",
       "                       -1.5555e-05, -8.5927e-07,  5.9871e-07, -1.4401e-06, -1.4003e-06,\n",
       "                        5.6966e-07,  3.0722e-06, -1.5542e-06,  2.2448e-06, -4.3207e-07,\n",
       "                        3.9695e-06,  6.2490e-06, -5.4025e-06,  6.5486e-06,  9.5260e-06,\n",
       "                        7.1364e-06, -4.0348e-06,  9.1746e-07, -2.5159e-06,  5.4789e-07,\n",
       "                       -2.4154e-06, -5.2570e-09, -7.2115e-07, -2.2548e-06, -8.0981e-08,\n",
       "                        7.6561e-06,  3.3623e-07,  1.5880e-06,  8.1805e-06,  8.0630e-06,\n",
       "                        1.4653e-06,  7.1577e-06, -3.1642e-06,  4.1760e-03, -6.4013e-03,\n",
       "                       -3.6482e-03,  9.2575e-03,  5.0115e-03,  5.4506e-03, -4.7436e-03,\n",
       "                       -4.7706e-04,  5.0157e-03, -5.3472e-03, -5.3726e-03, -5.1452e-03,\n",
       "                       -4.4114e-03, -4.7253e-03,  4.5780e-03, -6.7708e-03, -6.3111e-03,\n",
       "                       -7.1089e-03, -6.6470e-03,  4.7250e-03,  5.0749e-03, -3.1987e-03,\n",
       "                        5.1866e-03, -3.8301e-03,  4.8266e-03, -5.1400e-03,  5.2014e-03,\n",
       "                       -1.4637e-03, -5.6595e-03, -4.9792e-03, -5.5960e-03, -4.9605e-03,\n",
       "                       -4.6779e-03,  4.7547e-03,  5.1088e-03, -5.6356e-03,  6.1127e-03,\n",
       "                        5.4686e-03, -4.9314e-03, -5.0608e-03,  5.4198e-03,  4.6241e-03,\n",
       "                       -5.0850e-03,  5.1758e-03, -5.7944e-03, -5.2130e-03, -5.2369e-03,\n",
       "                       -5.8375e-03, -4.0006e-03, -9.8996e-03, -5.3890e-03, -5.7533e-03,\n",
       "                       -4.8468e-03, -4.3232e-03,  5.2193e-03, -4.3322e-03,  5.0769e-03,\n",
       "                        4.7776e-03, -5.1716e-03,  6.2039e-03, -5.4617e-03, -4.7131e-03,\n",
       "                       -4.9830e-03, -4.7327e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.5.attn.out_proj.weight',\n",
       "               tensor([[ 0.0090,  0.1239,  0.1062,  ...,  0.1037,  0.0817,  0.0299],\n",
       "                       [-0.0516, -0.0040, -0.0541,  ..., -0.0104, -0.0707, -0.0459],\n",
       "                       [ 0.0273,  0.0948, -0.0852,  ...,  0.1209, -0.0743,  0.0634],\n",
       "                       ...,\n",
       "                       [ 0.0581,  0.1028,  0.0059,  ..., -0.0181,  0.1122,  0.0711],\n",
       "                       [-0.0946,  0.1161,  0.0163,  ..., -0.0466, -0.1238, -0.0882],\n",
       "                       [-0.0112,  0.0316, -0.0284,  ...,  0.0947, -0.0875, -0.0504]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.attn.out_proj.bias',\n",
       "               tensor([ 0.0051, -0.0050, -0.0048, -0.0049, -0.0051, -0.0053, -0.0050, -0.0052,\n",
       "                        0.0049,  0.0051, -0.0052, -0.0048,  0.0048,  0.0054, -0.0061, -0.0058,\n",
       "                       -0.0052, -0.0047,  0.0055, -0.0052,  0.0069, -0.0039,  0.0053, -0.0063,\n",
       "                        0.0062,  0.0047,  0.0050,  0.0049, -0.0068,  0.0052, -0.0047,  0.0049,\n",
       "                       -0.0015,  0.0056,  0.0056,  0.0053, -0.0051, -0.0052, -0.0053,  0.0055,\n",
       "                       -0.0053, -0.0054, -0.0050,  0.0055, -0.0054, -0.0053,  0.0054,  0.0051,\n",
       "                        0.0049,  0.0050,  0.0051,  0.0054,  0.0061, -0.0053,  0.0057,  0.0053,\n",
       "                       -0.0055,  0.0053,  0.0058,  0.0047, -0.0049, -0.0058, -0.0054, -0.0046],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.mlp.0.weight',\n",
       "               tensor([[ 0.1054,  0.0760, -0.0542,  ...,  0.0433, -0.0800, -0.1203],\n",
       "                       [ 0.1013,  0.0513, -0.0191,  ..., -0.0037, -0.0549, -0.0842],\n",
       "                       [-0.0069, -0.0286,  0.0039,  ..., -0.0990, -0.0413,  0.0263],\n",
       "                       ...,\n",
       "                       [ 0.0379, -0.0151, -0.0656,  ...,  0.0661,  0.0310, -0.0626],\n",
       "                       [ 0.0714,  0.0421, -0.0511,  ..., -0.0403,  0.0741,  0.0557],\n",
       "                       [-0.0941,  0.0472,  0.0822,  ..., -0.0366, -0.1087,  0.0809]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.mlp.0.bias',\n",
       "               tensor([ 1.0933e-01,  9.5997e-02,  5.2767e-02,  1.0361e-02, -7.0093e-02,\n",
       "                       -7.7558e-02, -5.5030e-02, -7.3847e-02, -4.9360e-02, -8.8122e-02,\n",
       "                       -1.1476e-01, -2.1499e-02,  6.5412e-02,  1.2823e-01,  7.4319e-02,\n",
       "                        1.0202e-04, -4.3481e-02,  8.2961e-02, -2.4639e-02, -2.9208e-02,\n",
       "                        5.7256e-02,  5.3705e-02,  4.7338e-02, -1.1313e-01, -7.1126e-02,\n",
       "                        1.2112e-01, -6.3431e-02, -9.4837e-02,  1.8017e-02,  2.6037e-02,\n",
       "                       -1.1384e-01,  1.4633e-02,  1.5611e-03,  4.3327e-02,  8.1028e-02,\n",
       "                       -9.5667e-02, -8.5293e-02,  1.4351e-02, -1.2221e-01,  1.0418e-01,\n",
       "                       -1.1753e-01,  3.8893e-02,  4.2602e-02,  2.4961e-03,  8.9059e-02,\n",
       "                        1.0789e-01,  1.4818e-02, -8.8385e-02, -1.1148e-01, -6.1334e-02,\n",
       "                        7.7987e-02, -4.1040e-02, -4.6515e-02,  6.1786e-02, -8.1342e-02,\n",
       "                        3.6820e-03,  2.2785e-02, -7.3757e-02,  4.6981e-02,  5.2727e-03,\n",
       "                       -7.0815e-02, -1.2815e-01, -7.2084e-02, -1.1461e-01,  1.7531e-02,\n",
       "                        1.0739e-01,  9.2623e-02,  6.6230e-02,  9.2467e-02, -2.5118e-02,\n",
       "                       -2.6743e-02, -8.2472e-02,  1.0642e-02,  5.7410e-02, -4.8204e-02,\n",
       "                       -5.9369e-02, -4.7028e-02, -4.6226e-02, -6.8700e-02,  4.2779e-02,\n",
       "                       -3.7415e-02, -8.1303e-02, -1.2632e-01,  1.0388e-01, -8.5535e-02,\n",
       "                       -2.3845e-03, -2.2236e-02,  5.8275e-02,  9.9249e-04, -1.1263e-01,\n",
       "                        6.2279e-02,  6.6498e-02,  5.4801e-02, -6.9940e-03,  7.1431e-02,\n",
       "                       -3.7558e-02,  8.2836e-02, -5.0369e-02,  7.6638e-03,  5.7416e-03,\n",
       "                       -1.1829e-01,  9.8465e-02,  1.8690e-02, -1.1942e-01, -6.9129e-02,\n",
       "                       -3.1458e-02,  3.8742e-02, -2.0416e-02, -2.1199e-02, -5.5333e-02,\n",
       "                       -5.3501e-02,  3.1472e-02, -2.3690e-02,  6.1840e-02, -1.3181e-02,\n",
       "                        9.1475e-02,  5.2583e-02,  7.3437e-02,  3.3980e-02,  1.1756e-01,\n",
       "                       -6.5054e-02,  2.3067e-02,  9.8195e-03,  5.0629e-02,  8.2564e-02,\n",
       "                       -1.5114e-02, -6.3693e-02, -4.3304e-02,  1.0553e-01, -1.1218e-01,\n",
       "                       -6.4101e-02,  4.1359e-02,  1.0601e-01,  2.6229e-02, -2.7733e-02,\n",
       "                       -1.1147e-01,  3.9981e-02, -6.5549e-02,  8.9780e-02, -1.1245e-01,\n",
       "                       -1.0767e-01, -4.6775e-02, -6.3473e-02, -2.8545e-02,  8.3621e-02,\n",
       "                        7.0207e-02,  7.4764e-02, -1.0549e-01, -1.0246e-01, -6.9948e-02,\n",
       "                       -4.2859e-02, -5.5685e-02,  1.2072e-01,  4.5773e-02,  7.5254e-04,\n",
       "                       -2.2728e-02, -9.2536e-02, -1.2208e-01, -8.0460e-02,  1.0866e-01,\n",
       "                       -4.2723e-02, -6.1406e-02, -1.7937e-03,  3.0230e-02, -4.3649e-02,\n",
       "                       -9.7637e-02, -1.1127e-01,  4.5318e-02, -5.3302e-02,  1.0516e-01,\n",
       "                        9.7146e-04,  6.1446e-02,  2.0814e-02,  1.1907e-02, -8.7823e-02,\n",
       "                       -3.3454e-02, -3.5591e-02,  9.4999e-02,  9.1236e-02,  9.6906e-02,\n",
       "                       -5.8885e-02,  1.0110e-01, -3.9363e-02,  5.5241e-02,  8.2246e-02,\n",
       "                       -1.7467e-02, -3.3873e-02, -8.4416e-02, -7.7599e-02, -1.1376e-01,\n",
       "                        2.2899e-02, -4.0541e-02,  1.2157e-01,  6.5297e-03,  4.2935e-02,\n",
       "                        6.5354e-02, -9.1945e-02, -9.9204e-02,  3.7699e-03, -1.8128e-02,\n",
       "                        3.6350e-02, -1.1231e-02,  1.1121e-01,  7.9439e-02,  3.4719e-02,\n",
       "                       -7.3146e-02,  1.1138e-01,  4.0495e-02, -1.1976e-01, -6.1518e-02,\n",
       "                       -8.8711e-03,  3.1956e-02, -6.7615e-02, -3.8882e-02,  9.2554e-02,\n",
       "                       -1.0733e-01,  9.9504e-02,  1.4777e-03,  4.6685e-02,  1.8352e-02,\n",
       "                        2.1232e-02,  7.0608e-02,  8.2848e-02,  5.5081e-02, -1.3295e-02,\n",
       "                       -7.8165e-02, -3.5917e-03,  9.8242e-02, -1.0924e-01, -4.8617e-02,\n",
       "                        1.1351e-01,  2.2211e-02,  8.2766e-02,  3.0239e-02, -8.0647e-02,\n",
       "                       -8.7551e-02, -2.5530e-03,  8.6187e-02,  9.2511e-02,  5.1128e-03,\n",
       "                       -3.0421e-02, -1.0050e-02,  1.0738e-01,  1.0147e-01, -1.0443e-01,\n",
       "                        1.0174e-01,  8.9542e-03, -4.5215e-03, -4.6933e-02,  1.1082e-01,\n",
       "                        5.2323e-02, -3.0423e-02, -9.1826e-02,  9.7604e-02,  8.0917e-02,\n",
       "                        4.9023e-02], device='cuda:0')),\n",
       "              ('transformer.resblocks.5.mlp.2.weight',\n",
       "               tensor([[ 0.0379,  0.0196,  0.0498,  ...,  0.0380, -0.0599, -0.0614],\n",
       "                       [-0.0483,  0.0089, -0.0274,  ..., -0.0447,  0.0157,  0.0062],\n",
       "                       [ 0.0375,  0.0285, -0.0304,  ..., -0.0351, -0.0003,  0.0558],\n",
       "                       ...,\n",
       "                       [ 0.0529,  0.0087,  0.0332,  ...,  0.0669, -0.0047,  0.0521],\n",
       "                       [ 0.0509, -0.0519,  0.0543,  ..., -0.0141,  0.0344, -0.0134],\n",
       "                       [ 0.0470, -0.0001, -0.0383,  ...,  0.0291, -0.0091, -0.0286]],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.mlp.2.bias',\n",
       "               tensor([-0.0154, -0.0226,  0.0448, -0.0521,  0.0246, -0.0010,  0.0030,  0.0031,\n",
       "                        0.0083, -0.0292,  0.0267,  0.0570,  0.0641,  0.0537, -0.0009,  0.0309,\n",
       "                        0.0474, -0.0478,  0.0015,  0.0394, -0.0248, -0.0272,  0.0350,  0.0289,\n",
       "                       -0.0235,  0.0249, -0.0406,  0.0260, -0.0037, -0.0050, -0.0507,  0.0390,\n",
       "                        0.0514,  0.0622,  0.0422, -0.0011, -0.0109, -0.0175, -0.0518, -0.0532,\n",
       "                       -0.0344, -0.0191,  0.0273,  0.0354, -0.0571, -0.0657, -0.0170,  0.0373,\n",
       "                        0.0431,  0.0498,  0.0373,  0.0448, -0.0558, -0.0363,  0.0074,  0.0632,\n",
       "                       -0.0033, -0.0447, -0.0552, -0.0086,  0.0522,  0.0509, -0.0495,  0.0492],\n",
       "                      device='cuda:0')),\n",
       "              ('transformer.resblocks.5.ln_1.weight',\n",
       "               tensor([0.9981, 0.9961, 1.0041, 1.0068, 1.0081, 0.9996, 1.0004, 0.9948, 1.0034,\n",
       "                       0.9983, 0.9973, 1.0063, 1.0074, 1.0070, 0.9960, 1.0072, 1.0111, 0.9988,\n",
       "                       0.9968, 0.9946, 1.0031, 0.9954, 0.9948, 0.9972, 1.0132, 0.9989, 1.0020,\n",
       "                       0.9969, 0.9963, 0.9960, 0.9991, 1.0115, 0.9973, 1.0041, 1.0006, 0.9990,\n",
       "                       1.0085, 1.0082, 0.9990, 0.9963, 0.9968, 0.9970, 0.9993, 0.9961, 1.0041,\n",
       "                       0.9957, 1.0012, 0.9924, 1.0076, 1.0051, 0.9971, 0.9994, 1.0040, 0.9932,\n",
       "                       0.9993, 1.0035, 0.9963, 1.0039, 0.9961, 1.0060, 1.0103, 1.0063, 0.9962,\n",
       "                       0.9955], device='cuda:0')),\n",
       "              ('transformer.resblocks.5.ln_1.bias',\n",
       "               tensor([ 1.5903e-03,  5.8058e-03, -3.8888e-03,  1.4486e-03, -5.3189e-03,\n",
       "                       -4.7804e-03, -4.3074e-03,  4.9952e-03, -5.5997e-03,  4.4947e-03,\n",
       "                        1.9313e-03,  4.5258e-03,  5.5169e-03,  5.1783e-03, -5.2884e-03,\n",
       "                       -5.2573e-03, -5.0654e-03, -5.0038e-03, -3.7456e-03,  1.1166e-02,\n",
       "                        7.7051e-03,  1.4266e-03, -2.2155e-03, -9.8388e-03, -2.5911e-03,\n",
       "                        1.1468e-02,  5.2977e-03,  4.3842e-03,  3.9118e-03, -4.4032e-03,\n",
       "                       -5.0532e-03,  8.1790e-03,  4.9721e-03, -5.2558e-03,  4.9520e-03,\n",
       "                       -3.7401e-03, -5.4847e-03, -5.8178e-03, -4.9476e-03, -2.7384e-05,\n",
       "                       -3.6034e-03,  8.6701e-04, -4.3667e-05, -3.9676e-03, -4.9490e-03,\n",
       "                       -1.1556e-03, -5.1600e-03, -5.0423e-03,  6.4721e-03, -7.1269e-03,\n",
       "                        3.6602e-03,  5.6630e-03,  5.1015e-03,  5.8153e-03,  1.4096e-02,\n",
       "                       -6.0173e-03, -4.0023e-03, -5.8212e-03,  5.9880e-03,  6.3643e-03,\n",
       "                       -6.0019e-03,  3.8192e-03, -3.3091e-03, -4.2229e-03], device='cuda:0')),\n",
       "              ('transformer.resblocks.5.ln_2.weight',\n",
       "               tensor([0.9974, 1.0040, 0.9953, 1.0093, 0.9979, 1.0083, 1.0018, 0.9974, 1.0074,\n",
       "                       1.0046, 0.9999, 1.0066, 1.0069, 1.0121, 0.9983, 1.0041, 1.0073, 0.9951,\n",
       "                       1.0013, 0.9947, 0.9961, 1.0049, 0.9965, 0.9966, 0.9980, 1.0102, 1.0068,\n",
       "                       1.0002, 1.0024, 0.9977, 1.0059, 1.0069, 0.9980, 0.9975, 0.9957, 1.0040,\n",
       "                       1.0080, 1.0117, 0.9941, 1.0064, 0.9957, 1.0060, 1.0076, 0.9966, 0.9954,\n",
       "                       1.0048, 0.9955, 1.0076, 1.0110, 1.0115, 1.0020, 1.0007, 0.9964, 1.0082,\n",
       "                       0.9970, 0.9963, 1.0032, 1.0098, 0.9963, 1.0048, 1.0077, 1.0098, 1.0014,\n",
       "                       0.9961], device='cuda:0')),\n",
       "              ('transformer.resblocks.5.ln_2.bias',\n",
       "               tensor([-0.0026,  0.0052,  0.0050, -0.0071,  0.0030, -0.0068, -0.0028,  0.0030,\n",
       "                        0.0064,  0.0049, -0.0035,  0.0066,  0.0056,  0.0120,  0.0069, -0.0007,\n",
       "                       -0.0074,  0.0052, -0.0007,  0.0049, -0.0033, -0.0057, -0.0034, -0.0033,\n",
       "                       -0.0033,  0.0116,  0.0062,  0.0127,  0.0055, -0.0028, -0.0059,  0.0069,\n",
       "                       -0.0025,  0.0048, -0.0046, -0.0059, -0.0074, -0.0055,  0.0047,  0.0091,\n",
       "                       -0.0053, -0.0081, -0.0067,  0.0077,  0.0045, -0.0001, -0.0043,  0.0062,\n",
       "                        0.0062,  0.0052,  0.0055,  0.0005, -0.0006, -0.0062, -0.0033,  0.0025,\n",
       "                        0.0052,  0.0091, -0.0032,  0.0052, -0.0113, -0.0093,  0.0031,  0.0040],\n",
       "                      device='cuda:0')),\n",
       "              ('ln_post.weight',\n",
       "               tensor([1.0051, 0.9933, 1.0058, 1.0068, 1.0048, 0.9889, 0.9938, 1.0060, 1.0062,\n",
       "                       1.0086, 1.0136, 0.9932, 0.9868, 1.0058, 0.9960, 1.0013, 1.0102, 1.0017,\n",
       "                       1.0104, 1.0085, 0.9920, 0.9939, 0.9831, 0.9918, 0.9972, 1.0056, 1.0061,\n",
       "                       0.9924, 0.9991, 1.0058, 1.0043, 1.0051, 0.9971, 0.9970, 0.9922, 0.9939,\n",
       "                       1.0090, 0.9958, 1.0085, 0.9962, 0.9952, 1.0110, 1.0020, 0.9944, 0.9889,\n",
       "                       1.0091, 1.0075, 1.0080, 0.9954, 1.0079, 1.0109, 0.9999, 0.9938, 1.0083,\n",
       "                       1.0076, 0.9951, 0.9954, 0.9888, 0.9895, 1.0049, 1.0054, 1.0053, 0.9995,\n",
       "                       1.0055], device='cuda:0')),\n",
       "              ('ln_post.bias',\n",
       "               tensor([ 0.0061, -0.0060, -0.0059, -0.0061, -0.0063, -0.0067, -0.0060, -0.0068,\n",
       "                        0.0060,  0.0085, -0.0071, -0.0061,  0.0060,  0.0071, -0.0083, -0.0073,\n",
       "                       -0.0079, -0.0059,  0.0070, -0.0063,  0.0063,  0.0108,  0.0062, -0.0093,\n",
       "                        0.0077,  0.0058,  0.0063,  0.0062, -0.0087,  0.0063, -0.0057,  0.0062,\n",
       "                        0.0136,  0.0086,  0.0071,  0.0065, -0.0065, -0.0094, -0.0067,  0.0069,\n",
       "                       -0.0080, -0.0072, -0.0060,  0.0075, -0.0068, -0.0067,  0.0065,  0.0064,\n",
       "                        0.0065,  0.0067,  0.0067,  0.0067,  0.0059, -0.0072,  0.0071,  0.0066,\n",
       "                       -0.0066,  0.0066,  0.0070,  0.0060, -0.0061, -0.0080, -0.0065, -0.0052],\n",
       "                      device='cuda:0'))]),\n",
       " 'classifier': OrderedDict([('weight',\n",
       "               tensor([[ 0.0017,  0.0376, -0.0772,  ...,  0.0243,  0.0452, -0.0400],\n",
       "                       [ 0.0749,  0.0932,  0.0849,  ..., -0.0636, -0.0505, -0.0257],\n",
       "                       [ 0.0377, -0.2460, -0.1466,  ..., -0.0595,  0.1080, -0.1053],\n",
       "                       ...,\n",
       "                       [-0.1695, -0.0011,  0.2242,  ...,  0.2004,  0.0593,  0.1402],\n",
       "                       [-0.0759,  0.1754,  0.2237,  ...,  0.1313, -0.0049,  0.2154],\n",
       "                       [-0.2174,  0.1576,  0.0807,  ...,  0.0842,  0.1193,  0.1736]],\n",
       "                      device='cuda:0')),\n",
       "              ('bias',\n",
       "               tensor([-0.0541, -0.0503,  0.0065,  0.0608, -0.0425,  0.1059,  0.0360, -0.0283,\n",
       "                        0.1060, -0.0346, -0.2227, -0.0556, -0.1647, -0.1627, -0.0658, -0.1264,\n",
       "                        0.0110, -0.0888, -0.1182, -0.1783,  0.0045, -0.1245,  0.0173, -0.0030,\n",
       "                       -0.0612], device='cuda:0'))])}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a deep classifier in 5 epochs\n",
    "train_classification_model_head_only(vit_model, train_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
